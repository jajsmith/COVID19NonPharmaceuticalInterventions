{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Source Retrieval\n",
    "\n",
    "Done:\n",
    "- Ontario\n",
    "- Manitoba\n",
    "- British Columbia\n",
    "- New Brunswick\n",
    "\n",
    "TODO:\n",
    "- Nova Scotia\n",
    "- Northwest Territories\n",
    "- Nunavut\n",
    "- Yukon\n",
    "- Alberta\n",
    "- Saskatchewan\n",
    "- Prince Edward Island\n",
    "- Quebec\n",
    "- Newfoundland and Labrador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install bs4\n",
    "!python3 -m pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import feedparser\n",
    "from datetime import date\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020%2F07%2F08\n"
     ]
    }
   ],
   "source": [
    "today = date.today()\n",
    "today_str = str(today).replace('-', '%2F')\n",
    "\n",
    "print(today_str)\n",
    "country = 'Canada'\n",
    "src_cat = 'Government Website'\n",
    "columns = ['start_date', 'country', 'region', 'subregion', 'source_url', 'source_category', 'source_title', 'source_full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontario\n",
    "\n",
    "Since Ontario shows the most recent news on the first page, the range\n",
    "will need to continue to be expanded to capture all posts. Generally ~4 pages capture ~2 weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching page  1\n",
      "Searching page  2\n",
      "Searching page  3\n",
      "Searching page  4\n",
      "Searching page  5\n",
      "Searching page  6\n",
      "Searching page  7\n",
      "Searching page  8\n",
      "Searching page  9\n",
      "Searching page  10\n",
      "Searching page  11\n",
      "Searching page  12\n",
      "Searching page  13\n",
      "Searching page  14\n",
      "Searching page  15\n",
      "Searching page  16\n",
      "Searching page  17\n",
      "Searching page  18\n",
      "Searching page  19\n",
      "Searching page  20\n",
      "Searching page  21\n",
      "Searching page  22\n",
      "Searching page  23\n",
      "Searching page  24\n",
      "Searching page  25\n",
      "Searching page  26\n",
      "Searching page  27\n",
      "Searching page  28\n",
      "Searching page  29\n",
      "Searching page  30\n",
      "Searching page  31\n",
      "Searching page  32\n",
      "Searching page  33\n",
      "Searching page  34\n",
      "Searching page  35\n",
      "Searching page  36\n",
      "Searching page  37\n",
      "Searching page  38\n",
      "Searching page  39\n",
      "Searching page  40\n",
      "Searching page  41\n",
      "Searching page  42\n",
      "Searching page  43\n",
      "Searching page  44\n",
      "Searching page  45\n",
      "Searching page  46\n",
      "Searching page  47\n",
      "Searching page  48\n",
      "Searching page  49\n",
      "Searching page  50\n",
      "Searching page  51\n",
      "Searching page  52\n",
      "Searching page  53\n",
      "Searching page  54\n",
      "Searching page  55\n",
      "Searching page  56\n",
      "Searching page  57\n",
      "Searching page  58\n",
      "Searching page  59\n",
      "Searching page  60\n",
      "Searching page  61\n",
      "Searching page  62\n",
      "Searching page  63\n",
      "Searching page  64\n",
      "Searching page  65\n",
      "Searching page  66\n",
      "Searching page  67\n",
      "Searching page  68\n",
      "Searching page  69\n",
      "Searching page  70\n",
      "Searching page  71\n",
      "Searching page  72\n",
      "Searching page  73\n",
      "Searching page  74\n",
      "Searching page  75\n",
      "Searching page  76\n",
      "Searching page  77\n",
      "Searching page  78\n",
      "Searching page  79\n",
      "Searching page  80\n",
      "Searching page  81\n",
      "Searching page  82\n",
      "Searching page  83\n",
      "Searching page  84\n",
      "Searching page  85\n",
      "Searching page  86\n",
      "Searching page  87\n",
      "Searching page  88\n",
      "Searching page  89\n",
      "Searching page  90\n",
      "Searching page  91\n",
      "Searching page  92\n",
      "Searching page  93\n",
      "Searching page  94\n",
      "Searching page  95\n",
      "Searching page  96\n",
      "Searching page  97\n",
      "Searching page  98\n",
      "Searching page  99\n",
      "Searching page  100\n",
      "Searching page  101\n",
      "Searching page  102\n",
      "Searching page  103\n",
      "Searching page  104\n",
      "Searching page  105\n",
      "Searching page  106\n",
      "Searching page  107\n",
      "Searching page  108\n",
      "Searching page  109\n",
      "Searching page  110\n",
      "No articles found.\n",
      "CPU times: user 44.2 s, sys: 2.01 s, total: 46.2 s\n",
      "Wall time: 3min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "base_url = 'https://news.ontario.ca/en/search?content_type=all&utf8=%E2%9C%93&date_range_end=' + today_str + '&date_range_start=2020%2F01%2F01&date_select=desc&page='\n",
    "targets = [base_url + str(i) for i in range(1,4)]\n",
    "\n",
    "region = 'Ontario'\n",
    "subregion = ''\n",
    "\n",
    "# Specific structure for news.contario.ca/archive\n",
    "links = []\n",
    "page = 1\n",
    "while True:\n",
    "    print('Searching page ', page)\n",
    "    target = base_url + str(page)\n",
    "    \n",
    "    response = requests.get(target)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.findAll('article')\n",
    "    \n",
    "    if len(articles) == 0:\n",
    "        print('No articles found.')\n",
    "        break\n",
    "        \n",
    "    for article in articles:\n",
    "        smallersoup = BeautifulSoup(str(article), \"html.parser\")\n",
    "        link = smallersoup.findAll('a')[0]['href']\n",
    "        title = smallersoup.findAll('a')[0].string\n",
    "        pub_date = datetime.strptime(smallersoup.time.string.replace('.', ''), \"%B %d, %Y %I:%M %p\")\n",
    "\n",
    "        response = requests.get(link)\n",
    "        linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        full_text = linksoup.article.text\n",
    "\n",
    "        row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "        links.append(row)\n",
    "    \n",
    "    page += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(links, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1086, 8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['start_date'] = pd.to_datetime(df['start_date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sources/ontario.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manitoba\n",
    "\n",
    "Retrieve all news releases in 2020 for the Province of Manitoba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://news.gov.mb.ca'\n",
    "targets = [url_base + '/news/index.html?month=' + str(i) + '&year=2020&day=01&bgnG=GO&d=' for i in range(1,12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "region = 'Manitoba'\n",
    "subregion = ''\n",
    "\n",
    "links = []\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    if target.startswith(url_base): #manitoba\n",
    "        response = requests.get(target)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        items = soup.findAll(\"div\", {\"class\": \"maincontent\"})\n",
    "        smallersoup = BeautifulSoup(str(items), \"html.parser\")\n",
    "        for article in smallersoup.findAll('h2'):\n",
    "            a = article.a\n",
    "            relative_link = a['href']\n",
    "            link = url_base + relative_link.split('..')[-1]\n",
    "            title = a.string\n",
    "        \n",
    "            response = requests.get(link)\n",
    "            linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            date_text = linksoup.findAll(\"span\", {\"class\": \"article_date\"})[0].string\n",
    "            date = pd.to_datetime(date_text, format='%B %d, %Y')\n",
    "            pub_date = date.strftime('%m/%d/%Y')\n",
    "            \n",
    "            full_text = linksoup.findAll(\"div\", {\"class\": \"\"})[0].text\n",
    "\n",
    "            \n",
    "            row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "            links.append(row)\n",
    "            \n",
    "            # Get this link and copy full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(links, columns=columns)\n",
    "df.to_csv('sources/manitoba.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## British Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    'https://news.gov.bc.ca/Search?fromDate=2020%2F01%2F01&toDate=2020%2F05%2F26&q='\n",
    "region = 'British Columbia'\n",
    "subregion = ''\n",
    "\n",
    "query_url = 'https://news.gov.bc.ca/Search?FromDate=01/01/2020&Page='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "page = 1\n",
    "while True:\n",
    "    print(\"Page \", page)\n",
    "    target = query_url + str(page)\n",
    "    response = requests.get(target)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    items = soup.findAll(\"div\", {\"class\": \"article\"})\n",
    "    \n",
    "    if not items:\n",
    "        # No articles found\n",
    "        break\n",
    "    \n",
    "    for article in items:\n",
    "        smallersoup = BeautifulSoup(str(article), \"html.parser\")\n",
    "\n",
    "        #for article in smallersoup.findAll('div'):\n",
    "\n",
    "        title = smallersoup.a.string\n",
    "\n",
    "        date_text = smallersoup.findAll(\"div\", {\"class\" : \"item-date\"})[0].string\n",
    "        date = pd.to_datetime(date_text)\n",
    "        pub_date = date.strftime('%m/%d/%Y')\n",
    "\n",
    "        link = smallersoup.a['href']\n",
    "\n",
    "        response = requests.get(link)\n",
    "        linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        get_article = linksoup.findAll(\"article\")\n",
    "        if get_article:\n",
    "            full_text = get_article[0].text\n",
    "        else:\n",
    "            print(\"Couldn't retrieve full text for link: \", link)\n",
    "            full_text = \"\"\n",
    "\n",
    "        row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "        links.append(row)\n",
    "\n",
    "    page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(links, columns=columns)\n",
    "df.to_csv('sources/britishcolumbia.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a 2D array containing articles from New Brunsiwck since a specified start date, by default Jan 1 2020\n",
    "def new_brunswick_links(since=datetime(2020, 1, 1)):\n",
    "    \n",
    "    region = 'New Brunswick'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www2.gnb.ca/\"\n",
    "    url = url_base + \"content/gnb/en/news/recent_news.html?mainContent_par_newslist_start=\"\n",
    "    start = 0\n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        print(\"Page {}\".format(str(start // 25 + 1)))\n",
    "        request = requests.get(url + str(start))\n",
    "        soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "\n",
    "        article_div = soup.find('div', class_=\"none padded\")\n",
    "        article_soup = BeautifulSoup(str(article_div), 'html.parser')\n",
    "        articles = article_soup.find_all('li')\n",
    "\n",
    "        for article in articles:\n",
    "            small_soup = BeautifulSoup(str(article), 'html.parser')\n",
    "            ar_date_str = small_soup.find('span', class_=\"post_date\")\n",
    "            \n",
    "            if ar_date_str: # ensure list entry corresponds to dated article\n",
    "                # Date\n",
    "                ar_date = datetime.strptime(ar_date_str.text, \"%d %B %Y\")\n",
    "                \n",
    "                if ar_date < since: # only collect data after Jan 1 2020\n",
    "                    print(\"Stopping search at {}\".format(ar_date))\n",
    "                    return links \n",
    "                \n",
    "                a = article.a\n",
    "                # Title\n",
    "                title = a.text\n",
    "                # Body\n",
    "                relative_link = a['href']\n",
    "                link = url_base + relative_link\n",
    "                article_page = requests.get(link)\n",
    "                body_soup = BeautifulSoup(article_page.content, 'html.parser')\n",
    "                body = body_soup.find('div', class_=\"articleBody\").text\n",
    "                \n",
    "                row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "                links.append(row)\n",
    "#                 print(\"{}: {}\\n\".format(ar_date, title))\n",
    "                \n",
    "\n",
    "        start += 25 # articles per page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1\n",
      "Page 2\n",
      "Page 3\n",
      "Page 4\n",
      "Page 5\n",
      "Page 6\n",
      "Page 7\n",
      "Page 8\n",
      "Page 9\n",
      "Page 10\n",
      "Page 11\n",
      "Page 12\n",
      "Stopping search at 2019-12-30 00:00:00\n",
      "CPU times: user 22 s, sys: 653 ms, total: 22.7 s\n",
      "Wall time: 2min 58s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(287, 8)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.DataFrame(new_brunswick_links(), columns=columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sources/newbrunswick.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns a 2D array containing Nova Scotian articles since a specified start date, by default Jan 1 2020\n",
    "def load_nova_scotia(since=datetime(2020, 1, 1)):\n",
    "    \n",
    "    region = 'Nova Scotia'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://novascotia.ca/news\"\n",
    "    page = 1\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"/search/?page=\" + str(page)\n",
    "        print(\"Searching page {}\".format(page))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        titles = soup.find_all('dt', class_=\"RelTitle\")\n",
    "        summaries = soup.find_all('dd', class_=\"RelSummary\")\n",
    "        \n",
    "        for title, summary in zip(titles, summaries):\n",
    "            \n",
    "            if title['lang'] == \"fr\": continue\n",
    "                        \n",
    "            ar_date = datetime.strptime(summary.time.text, \"%B %d, %Y - %I:%M %p\")\n",
    "            \n",
    "            if ar_date < since:\n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return links\n",
    "            \n",
    "            relative_link = title.a['href'].split('..', 1)[1]\n",
    "            link = url_base + relative_link\n",
    "            \n",
    "            ar_request = requests.get(link)\n",
    "            ar_soup = BeautifulSoup(ar_request.content, 'html.parser')\n",
    "            body = ar_soup.find('div', {'id' : 'releaseBody'}).text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title.text, body]\n",
    "            links.append(row)\n",
    "\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching page 1\n",
      "Searching page 2\n",
      "Searching page 3\n",
      "Searching page 4\n",
      "Searching page 5\n",
      "Searching page 6\n",
      "Searching page 7\n",
      "Searching page 8\n",
      "Searching page 9\n",
      "Searching page 10\n",
      "Searching page 11\n",
      "Searching page 12\n",
      "Searching page 13\n",
      "Searching page 14\n",
      "Searching page 15\n",
      "Searching page 16\n",
      "Searching page 17\n",
      "Searching page 18\n",
      "Searching page 19\n",
      "Searching page 20\n",
      "Searching page 21\n",
      "Searching page 22\n",
      "Searching page 23\n",
      "Searching page 24\n",
      "Searching page 25\n",
      "Searching page 26\n",
      "Searching page 27\n",
      "Searching page 28\n",
      "Searching page 29\n",
      "Searching page 30\n",
      "Searching page 31\n",
      "Stopping search at date 2019-12-31 11:27:00\n",
      "CPU times: user 14.4 s, sys: 659 ms, total: 15.1 s\n",
      "Wall time: 8min 51s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(424, 8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.DataFrame(load_nova_scotia(), columns=columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sources/novascotia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
