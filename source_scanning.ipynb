{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Source Retrieval\n",
    "\n",
    "Done:\n",
    "- Ontario\n",
    "- Manitoba\n",
    "- British Columbia\n",
    "- New Brunswick\n",
    "- Nova Scotia\n",
    "- Northwest Territories\n",
    "- Saskatchewan\n",
    "\n",
    "TODO:\n",
    "- Nunavut\n",
    "- Yukon\n",
    "- Alberta\n",
    "- Prince Edward Island\n",
    "- Quebec\n",
    "- Newfoundland and Labrador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install bs4\n",
    "!python3 -m pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import feedparser\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'Canada'\n",
    "src_cat = 'Government Website'\n",
    "columns = ['start_date', 'country', 'region', 'subregion', 'source_url', 'source_category', 'source_title', 'source_full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontario\n",
    "\n",
    "Since Ontario shows the most recent news on the first page, the range\n",
    "will need to continue to be expanded to capture all posts. Generally ~4 pages capture ~2 weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ontario():\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \"\"\"\n",
    "\n",
    "    today = date.today()\n",
    "    today_str = str(today).replace('-', '%2F')\n",
    "\n",
    "    base_url = 'https://news.ontario.ca/en/search?content_type=all&utf8=%E2%9C%93&date_range_end=' + today_str + '&date_range_start=2020%2F01%2F01&date_select=desc&page='\n",
    "    targets = [base_url + str(i) for i in range(1,4)]\n",
    "\n",
    "    region = 'Ontario'\n",
    "    subregion = ''\n",
    "\n",
    "    # Specific structure for news.contario.ca/archive\n",
    "    links = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        print('Searching page ', page)\n",
    "        target = base_url + str(page)\n",
    "\n",
    "        response = requests.get(target)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        articles = soup.findAll('article')\n",
    "\n",
    "        if len(articles) == 0:\n",
    "            print('No articles found.')\n",
    "            return pd.DataFrame(links, columns=columns)\n",
    "\n",
    "        for article in articles:\n",
    "            smallersoup = BeautifulSoup(str(article), \"html.parser\")\n",
    "            link = smallersoup.findAll('a')[0]['href']\n",
    "            title = smallersoup.findAll('a')[0].string\n",
    "            pub_date = datetime.strptime(smallersoup.time.string.replace('.', ''), \"%B %d, %Y %I:%M %p\")\n",
    "\n",
    "            response = requests.get(link)\n",
    "            linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            full_text = linksoup.article.text\n",
    "\n",
    "            row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "            links.append(row)\n",
    "\n",
    "        page += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_ontario()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "df.to_csv('sources/ontario.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manitoba\n",
    "\n",
    "Retrieve all news releases in 2020 for the Province of Manitoba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_manitoba():\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \"\"\"\n",
    "\n",
    "    url_base = 'https://news.gov.mb.ca'\n",
    "    targets = [url_base + '/news/index.html?month=' + str(i) + '&year=2020&day=01&bgnG=GO&d=' for i in range(1,12)]\n",
    "\n",
    "    region = 'Manitoba'\n",
    "    subregion = ''\n",
    "\n",
    "    links = []\n",
    "    for target in targets:\n",
    "        print(target)\n",
    "        if target.startswith(url_base): #manitoba\n",
    "            response = requests.get(target)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            items = soup.findAll(\"div\", {\"class\": \"maincontent\"})\n",
    "            smallersoup = BeautifulSoup(str(items), \"html.parser\")\n",
    "            for article in smallersoup.findAll('h2'):\n",
    "                a = article.a\n",
    "                relative_link = a['href']\n",
    "                link = url_base + relative_link.split('..')[-1]\n",
    "                title = a.string\n",
    "\n",
    "                response = requests.get(link)\n",
    "                linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                date_text = linksoup.findAll(\"span\", {\"class\": \"article_date\"})[0].string\n",
    "                date = pd.to_datetime(date_text, format='%B %d, %Y')\n",
    "                pub_date = date.strftime('%m/%d/%Y')\n",
    "\n",
    "                full_text = linksoup.findAll(\"div\", {\"class\": \"\"})[0].text\n",
    "\n",
    "\n",
    "                row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "                links.append(row)\n",
    "\n",
    "                # Get this link and copy full text\n",
    "    return pd.DataFrame(links, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_manitoba()\n",
    "df.to_csv('sources/manitoba.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# British Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_british_columbia():\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'British Columbia'\n",
    "    subregion = ''\n",
    "\n",
    "    query_url = 'https://news.gov.bc.ca/Search?FromDate=01/01/2020&Page='\n",
    "    links = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        print(\"Page \", page)\n",
    "        target = query_url + str(page)\n",
    "        response = requests.get(target)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        items = soup.findAll(\"div\", {\"class\": \"article\"})\n",
    "\n",
    "        if not items:\n",
    "            return pd.DataFrame(links, columns=columns)\n",
    "\n",
    "        for article in items:\n",
    "            smallersoup = BeautifulSoup(str(article), \"html.parser\")\n",
    "\n",
    "            #for article in smallersoup.findAll('div'):\n",
    "\n",
    "            title = smallersoup.a.string\n",
    "\n",
    "            date_text = smallersoup.findAll(\"div\", {\"class\" : \"item-date\"})[0].string\n",
    "            date = pd.to_datetime(date_text)\n",
    "            pub_date = date.strftime('%m/%d/%Y')\n",
    "\n",
    "            link = smallersoup.a['href']\n",
    "\n",
    "            response = requests.get(link)\n",
    "            linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            get_article = linksoup.findAll(\"article\")\n",
    "            if get_article:\n",
    "                full_text = get_article[0].text\n",
    "            else:\n",
    "                print(\"Couldn't retrieve full text for link: \", link)\n",
    "                full_text = \"\"\n",
    "\n",
    "            row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "            links.append(row)\n",
    "\n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_british_columbia()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sources/britishcolumbia.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Brunswick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_new_brunswick(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    region = 'New Brunswick'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www2.gnb.ca/\"\n",
    "    url = url_base + \"content/gnb/en/news/recent_news.html?mainContent_par_newslist_start=\"\n",
    "    start = 0\n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        print(\"Page {}\".format(str(start // 25 + 1)))\n",
    "        request = requests.get(url + str(start))\n",
    "        soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "\n",
    "        article_div = soup.find('div', class_=\"none padded\")\n",
    "        article_soup = BeautifulSoup(str(article_div), 'html.parser')\n",
    "        articles = article_soup.find_all('li')\n",
    "\n",
    "        for article in articles:\n",
    "            small_soup = BeautifulSoup(str(article), 'html.parser')\n",
    "            ar_date_str = small_soup.find('span', class_=\"post_date\")\n",
    "            \n",
    "            if ar_date_str: # ensure list entry corresponds to dated article\n",
    "                # Date\n",
    "                ar_date = datetime.strptime(ar_date_str.text, \"%d %B %Y\")\n",
    "                \n",
    "                if ar_date < since: # only collect data after specified date\n",
    "                    print(\"Stopping search at date {}\".format(ar_date))\n",
    "                    return pd.DataFrame(links, columns=columns) \n",
    "                \n",
    "                a = article.a\n",
    "                # Title\n",
    "                title = a.text\n",
    "                # Body\n",
    "                relative_link = a['href']\n",
    "                link = url_base + relative_link\n",
    "                article_page = requests.get(link)\n",
    "                body_soup = BeautifulSoup(article_page.content, 'html.parser')\n",
    "                body = body_soup.find('div', class_=\"articleBody\").text\n",
    "                \n",
    "                row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "                links.append(row)\n",
    "#                 print(\"{}: {}\\n\".format(ar_date, title))\n",
    "                \n",
    "\n",
    "        start += 25 # articles per page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.DataFrame(load_new_brunswick(), columns=columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sources/newbrunswick.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nova Scotia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nova_scotia(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of Nova Scotia.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \n",
    "    \"\"\"\n",
    "    region = 'Nova Scotia'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://novascotia.ca/news\"\n",
    "    page = 1\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"/search/?page=\" + str(page)\n",
    "        print(\"Searching page {}\".format(page))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        titles = soup.find_all('dt', class_=\"RelTitle\")\n",
    "        summaries = soup.find_all('dd', class_=\"RelSummary\")\n",
    "        \n",
    "        for title, summary in zip(titles, summaries):\n",
    "            \n",
    "            if title['lang'] == \"fr\": continue\n",
    "                        \n",
    "            ar_date = datetime.strptime(summary.time.text, \"%B %d, %Y - %I:%M %p\")\n",
    "            \n",
    "            if ar_date < since:\n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(links, columns=columns)\n",
    "            \n",
    "            relative_link = title.a['href'].split('..', 1)[1]\n",
    "            link = url_base + relative_link\n",
    "            \n",
    "            ar_request = requests.get(link)\n",
    "            ar_soup = BeautifulSoup(ar_request.content, 'html.parser')\n",
    "            body = ar_soup.find('div', {'id' : 'releaseBody'}).text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title.text, body]\n",
    "            links.append(row)\n",
    "\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_nova_scotia()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sources/novascotia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_northwest_territories(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of the Northwest Territories.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \n",
    "    \"\"\"\n",
    "    region = 'Northwest Territories'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www.gov.nt.ca/\"\n",
    "    page = 0\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"en/newsroom?page=\" + str(page)\n",
    "        print(\"Searching page {}\".format(page + 1))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        ar_boxes = soup.find_all('div', class_ = re.compile('views-row')) # regex accounts for inconsistent `div` class names\n",
    "        \n",
    "        for box in ar_boxes:\n",
    "            boxed_soup = BeautifulSoup(str(box), 'html.parser') # parse each div\n",
    "            date_str = boxed_soup.find('span').text\n",
    "            ar_date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(links, columns=columns)\n",
    "            \n",
    "            title_a = boxed_soup.find('a')\n",
    "            title = title_a.text\n",
    "            relative_link = title_a['href']\n",
    "            \n",
    "            link = url_base + relative_link\n",
    "            ar_req = requests.get(link)\n",
    "            ar_soup = BeautifulSoup(ar_req.content, 'html.parser')\n",
    "            body = ar_soup.find('div', class_ = \"field-item even\").text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            links.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_northwest_territories()\n",
    "df.to_csv('sources/northwestterritories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saskatchewan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saskatchewan(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of Saskatchewan.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Saskatchewan'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www.saskatchewan.ca/government/news-and-media?page=\"\n",
    "    page = 1\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + str(page)\n",
    "        print(\"Searching page {}\".format(page))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        article_list = soup.find('ul', class_=\"results\")\n",
    "        article_soup = BeautifulSoup(str(article_list), 'html.parser')\n",
    "        list_items = article_soup.find_all('li')\n",
    "        \n",
    "        for item in list_items:\n",
    "            \n",
    "            date_str = item.time['datetime']\n",
    "            ar_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(links, columns=columns)\n",
    "            \n",
    "            title = item.a.text\n",
    "            link = item.a['href']\n",
    "            \n",
    "            body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "            body = body_soup.find('section', class_=\"general-content\").text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            links.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching page 1\n",
      "Searching page 2\n",
      "Searching page 3\n",
      "Searching page 4\n",
      "Searching page 5\n",
      "Searching page 6\n",
      "Searching page 7\n",
      "Searching page 8\n",
      "Searching page 9\n",
      "Searching page 10\n",
      "Searching page 11\n",
      "Searching page 12\n",
      "Searching page 13\n",
      "Searching page 14\n",
      "Searching page 15\n",
      "Searching page 16\n",
      "Searching page 17\n",
      "Searching page 18\n",
      "Searching page 19\n",
      "Searching page 20\n",
      "Searching page 21\n",
      "Searching page 22\n",
      "Searching page 23\n",
      "Searching page 24\n",
      "Searching page 25\n",
      "Searching page 26\n",
      "Searching page 27\n",
      "Searching page 28\n",
      "Searching page 29\n",
      "Searching page 30\n",
      "Searching page 31\n",
      "Searching page 32\n",
      "Searching page 33\n",
      "Searching page 34\n",
      "Searching page 35\n",
      "Searching page 36\n",
      "Searching page 37\n",
      "Searching page 38\n",
      "Searching page 39\n",
      "Searching page 40\n",
      "Searching page 41\n",
      "Searching page 42\n",
      "Searching page 43\n",
      "Searching page 44\n",
      "Searching page 45\n",
      "Searching page 46\n",
      "Searching page 47\n",
      "Searching page 48\n",
      "Stopping search at date 2019-12-27 00:00:00\n"
     ]
    }
   ],
   "source": [
    "df = load_saskatchewan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sources/saskatchewan.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
