{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Source Retrieval\n",
    "\n",
    "Done:\n",
    "- Ontario\n",
    "- Manitoba\n",
    "- British Columbia\n",
    "- New Brunswick\n",
    "- Nova Scotia\n",
    "- Northwest Territories\n",
    "- Saskatchewan\n",
    "\n",
    "TODO:\n",
    "- Nunavut\n",
    "- Yukon\n",
    "- Alberta\n",
    "- Prince Edward Island\n",
    "- Quebec\n",
    "- Newfoundland and Labrador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install bs4\n",
    "!python3 -m pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import feedparser\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'Canada'\n",
    "src_cat = 'Government Website'\n",
    "columns = ['start_date', 'country', 'region', 'subregion', 'source_url', 'source_category', 'source_title', 'source_full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontario\n",
    "\n",
    "Since Ontario shows the most recent news on the first page, the range\n",
    "will need to continue to be expanded to capture all posts. Generally ~4 pages capture ~2 weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ontario():\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \"\"\"\n",
    "\n",
    "    today = date.today()\n",
    "    today_str = str(today).replace('-', '%2F')\n",
    "\n",
    "    base_url = 'https://news.ontario.ca/en/search?content_type=all&utf8=%E2%9C%93&date_range_end=' + today_str + '&date_range_start=2020%2F01%2F01&date_select=desc&page='\n",
    "    targets = [base_url + str(i) for i in range(1,4)]\n",
    "\n",
    "    region = 'Ontario'\n",
    "    subregion = ''\n",
    "\n",
    "    # Specific structure for news.contario.ca/archive\n",
    "    links = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        print('Searching page ', page)\n",
    "        target = base_url + str(page)\n",
    "\n",
    "        response = requests.get(target)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        articles = soup.findAll('article')\n",
    "\n",
    "        if len(articles) == 0:\n",
    "            print('No articles found.')\n",
    "            return pd.DataFrame(links, columns=columns)\n",
    "\n",
    "        for article in articles:\n",
    "            smallersoup = BeautifulSoup(str(article), \"html.parser\")\n",
    "            link = smallersoup.findAll('a')[0]['href']\n",
    "            title = smallersoup.findAll('a')[0].string\n",
    "            pub_date = datetime.strptime(smallersoup.time.string.replace('.', ''), \"%B %d, %Y %I:%M %p\")\n",
    "\n",
    "            response = requests.get(link)\n",
    "            linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            full_text = linksoup.article.text\n",
    "\n",
    "            row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "            links.append(row)\n",
    "\n",
    "        page += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_ontario()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape\n",
    "df.to_csv('sources/ontario.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manitoba\n",
    "\n",
    "Retrieve all news releases in 2020 for the Province of Manitoba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_manitoba():\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \"\"\"\n",
    "\n",
    "    url_base = 'https://news.gov.mb.ca'\n",
    "    targets = [url_base + '/news/index.html?month=' + str(i) + '&year=2020&day=01&bgnG=GO&d=' for i in range(1,12)]\n",
    "\n",
    "    region = 'Manitoba'\n",
    "    subregion = ''\n",
    "\n",
    "    links = []\n",
    "    for target in targets:\n",
    "        print(target)\n",
    "        if target.startswith(url_base): #manitoba\n",
    "            response = requests.get(target)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            items = soup.findAll(\"div\", {\"class\": \"maincontent\"})\n",
    "            smallersoup = BeautifulSoup(str(items), \"html.parser\")\n",
    "            for article in smallersoup.findAll('h2'):\n",
    "                a = article.a\n",
    "                relative_link = a['href']\n",
    "                link = url_base + relative_link.split('..')[-1]\n",
    "                title = a.string\n",
    "\n",
    "                response = requests.get(link)\n",
    "                linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                date_text = linksoup.findAll(\"span\", {\"class\": \"article_date\"})[0].string\n",
    "                date = pd.to_datetime(date_text, format='%B %d, %Y')\n",
    "                pub_date = date.strftime('%m/%d/%Y')\n",
    "\n",
    "                full_text = linksoup.findAll(\"div\", {\"class\": \"\"})[0].text\n",
    "\n",
    "\n",
    "                row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "                links.append(row)\n",
    "\n",
    "                # Get this link and copy full text\n",
    "    return pd.DataFrame(links, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_manitoba()\n",
    "df.to_csv('sources/manitoba.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# British Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_british_columbia():\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'British Columbia'\n",
    "    subregion = ''\n",
    "\n",
    "    query_url = 'https://news.gov.bc.ca/Search?FromDate=01/01/2020&Page='\n",
    "    links = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        print(\"Page \", page)\n",
    "        target = query_url + str(page)\n",
    "        response = requests.get(target)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        items = soup.findAll(\"div\", {\"class\": \"article\"})\n",
    "\n",
    "        if not items:\n",
    "            return pd.DataFrame(links, columns=columns)\n",
    "\n",
    "        for article in items:\n",
    "            smallersoup = BeautifulSoup(str(article), \"html.parser\")\n",
    "\n",
    "            #for article in smallersoup.findAll('div'):\n",
    "\n",
    "            title = smallersoup.a.string\n",
    "\n",
    "            date_text = smallersoup.findAll(\"div\", {\"class\" : \"item-date\"})[0].string\n",
    "            date = pd.to_datetime(date_text)\n",
    "            pub_date = date.strftime('%m/%d/%Y')\n",
    "\n",
    "            link = smallersoup.a['href']\n",
    "\n",
    "            response = requests.get(link)\n",
    "            linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            get_article = linksoup.findAll(\"article\")\n",
    "            if get_article:\n",
    "                full_text = get_article[0].text\n",
    "            else:\n",
    "                print(\"Couldn't retrieve full text for link: \", link)\n",
    "                full_text = \"\"\n",
    "\n",
    "            row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "            links.append(row)\n",
    "\n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_british_columbia()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sources/britishcolumbia.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Brunswick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_new_brunswick(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    region = 'New Brunswick'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www2.gnb.ca/\"\n",
    "    url = url_base + \"content/gnb/en/news/recent_news.html?mainContent_par_newslist_start=\"\n",
    "    start = 0\n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        print(\"Page {}\".format(str(start // 25 + 1)))\n",
    "        request = requests.get(url + str(start))\n",
    "        soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "\n",
    "        article_div = soup.find('div', class_=\"none padded\")\n",
    "        article_soup = BeautifulSoup(str(article_div), 'html.parser')\n",
    "        articles = article_soup.find_all('li')\n",
    "\n",
    "        for article in articles:\n",
    "            small_soup = BeautifulSoup(str(article), 'html.parser')\n",
    "            ar_date_str = small_soup.find('span', class_=\"post_date\")\n",
    "            \n",
    "            if ar_date_str: # ensure list entry corresponds to dated article\n",
    "                # Date\n",
    "                ar_date = datetime.strptime(ar_date_str.text, \"%d %B %Y\")\n",
    "                \n",
    "                if ar_date < since: # only collect data after specified date\n",
    "                    print(\"Stopping search at date {}\".format(ar_date))\n",
    "                    return pd.DataFrame(links, columns=columns) \n",
    "                \n",
    "                a = article.a\n",
    "                # Title\n",
    "                title = a.text\n",
    "                # Body\n",
    "                relative_link = a['href']\n",
    "                link = url_base + relative_link\n",
    "                article_page = requests.get(link)\n",
    "                body_soup = BeautifulSoup(article_page.content, 'html.parser')\n",
    "                body = body_soup.find('div', class_=\"articleBody\").text\n",
    "                \n",
    "                row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "                links.append(row)\n",
    "#                 print(\"{}: {}\\n\".format(ar_date, title))\n",
    "                \n",
    "\n",
    "        start += 25 # articles per page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.DataFrame(load_new_brunswick(), columns=columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sources/newbrunswick.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nova Scotia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nova_scotia(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of Nova Scotia.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \n",
    "    \"\"\"\n",
    "    region = 'Nova Scotia'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://novascotia.ca/news\"\n",
    "    page = 1\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"/search/?page=\" + str(page)\n",
    "        print(\"Searching page {}\".format(page))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        titles = soup.find_all('dt', class_=\"RelTitle\")\n",
    "        summaries = soup.find_all('dd', class_=\"RelSummary\")\n",
    "        \n",
    "        for title, summary in zip(titles, summaries):\n",
    "            \n",
    "            if title['lang'] == \"fr\": continue\n",
    "                        \n",
    "            ar_date = datetime.strptime(summary.time.text, \"%B %d, %Y - %I:%M %p\")\n",
    "            \n",
    "            if ar_date < since:\n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(links, columns=columns)\n",
    "            \n",
    "            relative_link = title.a['href'].split('..', 1)[1]\n",
    "            link = url_base + relative_link\n",
    "            \n",
    "            ar_request = requests.get(link)\n",
    "            ar_soup = BeautifulSoup(ar_request.content, 'html.parser')\n",
    "            body = ar_soup.find('div', {'id' : 'releaseBody'}).text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title.text, body]\n",
    "            links.append(row)\n",
    "\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_nova_scotia()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sources/novascotia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_northwest_territories(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of the Northwest Territories.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \n",
    "    \"\"\"\n",
    "    region = 'Northwest Territories'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www.gov.nt.ca/\"\n",
    "    page = 0\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"en/newsroom?page=\" + str(page)\n",
    "        print(\"Searching page {}\".format(page + 1))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        ar_boxes = soup.find_all('div', class_ = re.compile('views-row')) # regex accounts for inconsistent `div` class names\n",
    "        \n",
    "        for box in ar_boxes:\n",
    "            boxed_soup = BeautifulSoup(str(box), 'html.parser') # parse each div\n",
    "            date_str = boxed_soup.find('span').text\n",
    "            ar_date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(links, columns=columns)\n",
    "            \n",
    "            title_a = boxed_soup.find('a')\n",
    "            title = title_a.text\n",
    "            relative_link = title_a['href']\n",
    "            \n",
    "            link = url_base + relative_link\n",
    "            ar_req = requests.get(link)\n",
    "            ar_soup = BeautifulSoup(ar_req.content, 'html.parser')\n",
    "            body = ar_soup.find('div', class_ = \"field-item even\").text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            links.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_northwest_territories()\n",
    "df.to_csv('sources/northwestterritories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saskatchewan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saskatchewan(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of Saskatchewan.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Saskatchewan'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www.saskatchewan.ca/government/news-and-media?page=\"\n",
    "    page = 1\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + str(page)\n",
    "        print(\"Searching page {}\".format(page))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        article_list = soup.find('ul', class_=\"results\")\n",
    "        article_soup = BeautifulSoup(str(article_list), 'html.parser')\n",
    "        list_items = article_soup.find_all('li')\n",
    "        \n",
    "        for item in list_items:\n",
    "            \n",
    "            date_str = item.time['datetime']\n",
    "            ar_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(links, columns=columns)\n",
    "            \n",
    "            title = item.a.text\n",
    "            link = item.a['href']\n",
    "            \n",
    "            body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "            body = body_soup.find('section', class_=\"general-content\").text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            links.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_saskatchewan()\n",
    "df.to_csv('sources/saskatchewan.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nunavut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nunavut(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of Nunavut.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Nunavut'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://gov.nu.ca\"\n",
    "    page = 0\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"/news?page=\" + str(page)\n",
    "        print(\"Searching page {}\".format(page + 1))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        main_section = soup.find('section', {\"id\" : \"block-system-main\"})\n",
    "        main_section_soup = BeautifulSoup(str(main_section), 'html.parser')\n",
    "        \n",
    "        divs = main_section_soup.find_all('div', re.compile('views-row(.*)'))\n",
    "        \n",
    "        for div in divs:\n",
    "            \n",
    "            div_soup = BeautifulSoup(str(div), 'html.parser')\n",
    "            date_str = div_soup.find('span', class_=\"date-display-single\").text\n",
    "            ar_date = datetime.strptime(date_str, \"%d %B %Y\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(links, columns=columns)\n",
    "            \n",
    "            a = div_soup.find('a')\n",
    "            title = a.text\n",
    "            link = url_base + a['href']\n",
    "                        \n",
    "            body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "            body = body_soup.find('div', class_=\"region region-content\").text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            links.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_nunavut()\n",
    "df.to_csv('sources/nunavut.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yukon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yukon(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of the Yukon.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Yukon'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://yukon.ca\"\n",
    "    page = 0\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"/news?page=\" + str(page)\n",
    "        print(\"Searching page {}\".format(page + 1))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        main_div = soup.find('div', class_ = \"view-content\")\n",
    "        main_div_soup = BeautifulSoup(str(main_div), 'html.parser')\n",
    "        \n",
    "        divs = main_div_soup.find_all('div', re.compile('views-row(.*)'))\n",
    "        \n",
    "        for div in divs:\n",
    "            \n",
    "            div_soup = BeautifulSoup(str(div), 'html.parser')\n",
    "            date_str = div_soup.find('small').text\n",
    "            ar_date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(links, columns=columns)\n",
    "            \n",
    "            a = div_soup.find('a')\n",
    "            title = a.text\n",
    "            link = url_base + a['href']\n",
    "                        \n",
    "            body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "            body = body_soup.find('div', class_=\"region region-content\").text\n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            links.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching page 1\n",
      "Searching page 2\n",
      "Searching page 3\n",
      "Searching page 4\n",
      "Searching page 5\n",
      "Searching page 6\n",
      "Searching page 7\n",
      "Searching page 8\n",
      "Searching page 9\n",
      "Searching page 10\n",
      "Searching page 11\n",
      "Searching page 12\n",
      "Searching page 13\n",
      "Searching page 14\n",
      "Searching page 15\n",
      "Searching page 16\n",
      "Searching page 17\n",
      "Searching page 18\n",
      "Searching page 19\n",
      "Searching page 20\n",
      "Stopping search at date 2019-12-31 00:00:00\n",
      "CPU times: user 1min 6s, sys: 1.13 s, total: 1min 8s\n",
      "Wall time: 4min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = load_yukon()\n",
    "df.to_csv('sources/yukon.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
