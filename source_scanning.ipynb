{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Source Retrieval\n",
    "\n",
    "Done:\n",
    "- Ontario\n",
    "- Manitoba\n",
    "- British Columbia\n",
    "- New Brunswick\n",
    "- Nova Scotia\n",
    "- Northwest Territories\n",
    "- Saskatchewan\n",
    "- Nunavut\n",
    "- Yukon\n",
    "- Prince Edward Island\n",
    "\n",
    "TODO:\n",
    "- Alberta\n",
    "- Quebec\n",
    "- Newfoundland and Labrador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install bs4\n",
    "!python3 -m pip install feedparser\n",
    "!python3 -m pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import feedparser\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'Canada'\n",
    "src_cat = 'Government Website'\n",
    "columns = ['start_date', 'country', 'region', 'subregion', 'source_url', 'source_category', 'source_title', 'source_full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontario\n",
    "\n",
    "Since Ontario shows the most recent news on the first page, the range\n",
    "will need to continue to be expanded to capture all posts. Generally ~4 pages capture ~2 weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ontario():\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \"\"\"\n",
    "\n",
    "    today = date.today()\n",
    "    today_str = str(today).replace('-', '%2F')\n",
    "\n",
    "    base_url = 'https://news.ontario.ca/en/search?content_type=all&utf8=%E2%9C%93&date_range_end=' + today_str + '&date_range_start=2020%2F01%2F01&date_select=desc&page='\n",
    "    targets = [base_url + str(i) for i in range(1,4)]\n",
    "\n",
    "    region = 'Ontario'\n",
    "    subregion = ''\n",
    "\n",
    "    # Specific structure for news.contario.ca/archive\n",
    "    links = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        print('Searching page ', page)\n",
    "        target = base_url + str(page)\n",
    "\n",
    "        response = requests.get(target)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        articles = soup.findAll('article')\n",
    "\n",
    "        if len(articles) == 0:\n",
    "            print('No articles found.')\n",
    "            return pd.DataFrame(links, columns=columns)\n",
    "\n",
    "        for article in articles:\n",
    "            smallersoup = BeautifulSoup(str(article), \"html.parser\")\n",
    "            link = smallersoup.findAll('a')[0]['href']\n",
    "            title = smallersoup.findAll('a')[0].string\n",
    "            pub_date = datetime.strptime(smallersoup.time.string.replace('.', ''), \"%B %d, %Y %I:%M %p\")\n",
    "\n",
    "            response = requests.get(link)\n",
    "            linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            full_text = linksoup.article.text\n",
    "\n",
    "            row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "            links.append(row)\n",
    "\n",
    "        page += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching page  1\n",
      "Searching page  2\n",
      "Searching page  3\n",
      "Searching page  4\n",
      "Searching page  5\n",
      "Searching page  6\n",
      "Searching page  7\n",
      "Searching page  8\n",
      "Searching page  9\n",
      "Searching page  10\n",
      "Searching page  11\n",
      "Searching page  12\n",
      "Searching page  13\n",
      "Searching page  14\n",
      "Searching page  15\n",
      "Searching page  16\n",
      "Searching page  17\n",
      "Searching page  18\n",
      "Searching page  19\n",
      "Searching page  20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-df3a19c0d2d6>\u001b[0m in \u001b[0;36mload_ontario\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mpub_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmallersoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%B %d, %Y %I:%M %p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mlinksoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mfull_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinksoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    424\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    419\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 669\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    670\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1241\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = load_ontario()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-bc395d506abf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sources/ontario.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.shape\n",
    "df.to_csv('sources/ontario.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manitoba\n",
    "\n",
    "Retrieve all news releases in 2020 for the Province of Manitoba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_manitoba():\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \"\"\"\n",
    "\n",
    "    url_base = 'https://news.gov.mb.ca'\n",
    "    targets = [url_base + '/news/index.html?month=' + str(i) + '&year=2020&day=01&bgnG=GO&d=' for i in range(1,12)]\n",
    "\n",
    "    region = 'Manitoba'\n",
    "    subregion = ''\n",
    "\n",
    "    links = []\n",
    "    for target in targets:\n",
    "        print(target)\n",
    "        if target.startswith(url_base): #manitoba\n",
    "            response = requests.get(target)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            items = soup.findAll(\"div\", {\"class\": \"maincontent\"})\n",
    "            smallersoup = BeautifulSoup(str(items), \"html.parser\")\n",
    "            for article in smallersoup.findAll('h2'):\n",
    "                a = article.a\n",
    "                relative_link = a['href']\n",
    "                link = url_base + relative_link.split('..')[-1]\n",
    "                title = a.string\n",
    "\n",
    "                response = requests.get(link)\n",
    "                linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                date_text = linksoup.findAll(\"span\", {\"class\": \"article_date\"})[0].string\n",
    "                date = pd.to_datetime(date_text, format='%B %d, %Y')\n",
    "                pub_date = date.strftime('%m/%d/%Y')\n",
    "\n",
    "                full_text = linksoup.findAll(\"div\", {\"class\": \"\"})[0].text\n",
    "\n",
    "\n",
    "                row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "                links.append(row)\n",
    "\n",
    "                # Get this link and copy full text\n",
    "    return pd.DataFrame(links, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_manitoba()\n",
    "df.to_csv('sources/manitoba.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# British Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_british_columbia():\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'British Columbia'\n",
    "    subregion = ''\n",
    "\n",
    "    query_url = 'https://news.gov.bc.ca/Search?FromDate=01/01/2020&Page='\n",
    "    links = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        print(\"Page \", page)\n",
    "        target = query_url + str(page)\n",
    "        response = requests.get(target)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        items = soup.findAll(\"div\", {\"class\": \"article\"})\n",
    "\n",
    "        if not items:\n",
    "            return pd.DataFrame(links, columns=columns)\n",
    "\n",
    "        for article in items:\n",
    "            smallersoup = BeautifulSoup(str(article), \"html.parser\")\n",
    "\n",
    "            #for article in smallersoup.findAll('div'):\n",
    "\n",
    "            title = smallersoup.a.string\n",
    "\n",
    "            date_text = smallersoup.findAll(\"div\", {\"class\" : \"item-date\"})[0].string\n",
    "            date = pd.to_datetime(date_text)\n",
    "            pub_date = date.strftime('%m/%d/%Y')\n",
    "\n",
    "            link = smallersoup.a['href']\n",
    "\n",
    "            response = requests.get(link)\n",
    "            linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            get_article = linksoup.findAll(\"article\")\n",
    "            if get_article:\n",
    "                full_text = get_article[0].text\n",
    "            else:\n",
    "                print(\"Couldn't retrieve full text for link: \", link)\n",
    "                full_text = \"\"\n",
    "\n",
    "            row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "            links.append(row)\n",
    "\n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_british_columbia()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sources/britishcolumbia.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Brunswick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_new_brunswick(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    region = 'New Brunswick'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www2.gnb.ca/\"\n",
    "    url = url_base + \"content/gnb/en/news/recent_news.html?mainContent_par_newslist_start=\"\n",
    "    start = 0\n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        print(\"Page {}\".format(str(start // 25 + 1)))\n",
    "        request = requests.get(url + str(start))\n",
    "        soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "\n",
    "        article_div = soup.find('div', class_=\"none padded\")\n",
    "        article_soup = BeautifulSoup(str(article_div), 'html.parser')\n",
    "        articles = article_soup.find_all('li')\n",
    "\n",
    "        for article in articles:\n",
    "            small_soup = BeautifulSoup(str(article), 'html.parser')\n",
    "            ar_date_str = small_soup.find('span', class_=\"post_date\")\n",
    "            \n",
    "            if ar_date_str: # ensure list entry corresponds to dated article\n",
    "                # Date\n",
    "                ar_date = datetime.strptime(ar_date_str.text, \"%d %B %Y\")\n",
    "                \n",
    "                if ar_date < since: # only collect data after specified date\n",
    "                    print(\"Stopping search at date {}\".format(ar_date))\n",
    "                    return pd.DataFrame(links, columns=columns) \n",
    "                \n",
    "                a = article.a\n",
    "                # Title\n",
    "                title = a.text\n",
    "                # Body\n",
    "                relative_link = a['href']\n",
    "                link = url_base + relative_link\n",
    "                article_page = requests.get(link)\n",
    "                body_soup = BeautifulSoup(article_page.content, 'html.parser')\n",
    "                body = body_soup.find('div', class_=\"articleBody\").text\n",
    "                \n",
    "                row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "                links.append(row)\n",
    "#                 print(\"{}: {}\\n\".format(ar_date, title))\n",
    "                \n",
    "\n",
    "        start += 25 # articles per page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.DataFrame(load_new_brunswick(), columns=columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sources/newbrunswick.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nova Scotia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nova_scotia(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of Nova Scotia.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \n",
    "    \"\"\"\n",
    "    region = 'Nova Scotia'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://novascotia.ca/news\"\n",
    "    page = 1\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"/search/?page=\" + str(page)\n",
    "        print(\"Searching page {}\".format(page))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        titles = soup.find_all('dt', class_=\"RelTitle\")\n",
    "        summaries = soup.find_all('dd', class_=\"RelSummary\")\n",
    "        \n",
    "        for title, summary in zip(titles, summaries):\n",
    "            \n",
    "            if title['lang'] == \"fr\": continue\n",
    "                        \n",
    "            ar_date = datetime.strptime(summary.time.text, \"%B %d, %Y - %I:%M %p\")\n",
    "            \n",
    "            if ar_date < since:\n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(links, columns=columns)\n",
    "            \n",
    "            relative_link = title.a['href'].split('..', 1)[1]\n",
    "            link = url_base + relative_link\n",
    "            \n",
    "            ar_request = requests.get(link)\n",
    "            ar_soup = BeautifulSoup(ar_request.content, 'html.parser')\n",
    "            body = ar_soup.find('div', {'id' : 'releaseBody'}).text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title.text, body]\n",
    "            links.append(row)\n",
    "\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_nova_scotia()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sources/novascotia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_northwest_territories(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of the Northwest Territories.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \n",
    "    \"\"\"\n",
    "    region = 'Northwest Territories'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www.gov.nt.ca/\"\n",
    "    page = 0\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"en/newsroom?page=\" + str(page)\n",
    "        print(\"Searching page {}\".format(page + 1))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        ar_boxes = soup.find_all('div', class_ = re.compile('views-row')) # regex accounts for inconsistent `div` class names\n",
    "        \n",
    "        for box in ar_boxes:\n",
    "            boxed_soup = BeautifulSoup(str(box), 'html.parser') # parse each div\n",
    "            date_str = boxed_soup.find('span').text\n",
    "            ar_date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(links, columns=columns)\n",
    "            \n",
    "            title_a = boxed_soup.find('a')\n",
    "            title = title_a.text\n",
    "            relative_link = title_a['href']\n",
    "            \n",
    "            link = url_base + relative_link\n",
    "            ar_req = requests.get(link)\n",
    "            ar_soup = BeautifulSoup(ar_req.content, 'html.parser')\n",
    "            body = ar_soup.find('div', class_ = \"field-item even\").text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            links.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_northwest_territories()\n",
    "df.to_csv('sources/northwestterritories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saskatchewan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saskatchewan(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of Saskatchewan.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Saskatchewan'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www.saskatchewan.ca/government/news-and-media?page=\"\n",
    "    page = 1\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + str(page)\n",
    "        print(\"Searching page {}\".format(page))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        article_list = soup.find('ul', class_=\"results\")\n",
    "        article_soup = BeautifulSoup(str(article_list), 'html.parser')\n",
    "        list_items = article_soup.find_all('li')\n",
    "        \n",
    "        for item in list_items:\n",
    "            \n",
    "            date_str = item.time['datetime']\n",
    "            ar_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(links, columns=columns)\n",
    "            \n",
    "            title = item.a.text\n",
    "            link = item.a['href']\n",
    "            \n",
    "            body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "            body = body_soup.find('section', class_=\"general-content\").text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            links.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_saskatchewan()\n",
    "df.to_csv('sources/saskatchewan.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nunavut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nunavut(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of Nunavut.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Nunavut'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://gov.nu.ca\"\n",
    "    page = 0\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"/news?page=\" + str(page)\n",
    "        print(\"Searching page {}\".format(page + 1))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        main_section = soup.find('section', {\"id\" : \"block-system-main\"})\n",
    "        main_section_soup = BeautifulSoup(str(main_section), 'html.parser')\n",
    "        \n",
    "        divs = main_section_soup.find_all('div', re.compile('views-row(.*)'))\n",
    "        \n",
    "        for div in divs:\n",
    "            \n",
    "            div_soup = BeautifulSoup(str(div), 'html.parser')\n",
    "            date_str = div_soup.find('span', class_=\"date-display-single\").text\n",
    "            ar_date = datetime.strptime(date_str, \"%d %B %Y\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(links, columns=columns)\n",
    "            \n",
    "            a = div_soup.find('a')\n",
    "            title = a.text\n",
    "            link = url_base + a['href']\n",
    "                        \n",
    "            body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "            body = body_soup.find('div', class_=\"region region-content\").text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            links.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_nunavut()\n",
    "df.to_csv('sources/nunavut.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yukon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yukon(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of the Yukon.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Yukon'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://yukon.ca\"\n",
    "    page = 0\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"/news?page=\" + str(page)\n",
    "        print(\"Searching page {}\".format(page + 1))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        main_div = soup.find('div', class_ = \"view-content\")\n",
    "        main_div_soup = BeautifulSoup(str(main_div), 'html.parser')\n",
    "        \n",
    "        divs = main_div_soup.find_all('div', re.compile('views-row(.*)'))\n",
    "        \n",
    "        for div in divs:\n",
    "            \n",
    "            div_soup = BeautifulSoup(str(div), 'html.parser')\n",
    "            date_str = div_soup.find('small').text\n",
    "            ar_date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(links, columns=columns)\n",
    "            \n",
    "            a = div_soup.find('a')\n",
    "            title = a.text\n",
    "            link = url_base + a['href']\n",
    "                        \n",
    "            body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "            body = body_soup.find('div', class_=\"region region-content\").text\n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            links.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_yukon()\n",
    "df.to_csv('sources/yukon.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prince Edward Island"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pei(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of Prince Edward Island.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Prince Edward Island'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www.princeedwardisland.ca\"\n",
    "    page = 0\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"/news?page=\" + str(page)\n",
    "        print(\"Searching page {}\".format(page + 1))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        divs = soup.find_all('div', class_=\"right content views-fieldset\")\n",
    "        \n",
    "        for div in divs:\n",
    "                        \n",
    "            div_soup = BeautifulSoup(str(div), 'html.parser')\n",
    "            date_str = div_soup.find('div', class_=\"date\").text\n",
    "            ar_date = datetime.strptime(date_str, \"%A, %B %d, %Y\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(links, columns=columns)\n",
    "            \n",
    "            a = div_soup.find('a')\n",
    "            title = a.text\n",
    "            link = url_base + a['href']\n",
    "            \n",
    "            body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "            body = body_soup.find('div', class_=\"maincontentmain\").text\n",
    "                        \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            links.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_pei()\n",
    "df.to_csv('sources/pei.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_alberta(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of Alberta.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Alberta'\n",
    "    sub_region = ''\n",
    "    \n",
    "    days_back = (datetime.today() - since).days\n",
    "    url = \"https://www.alberta.ca/NewsRoom/newsroom.cfm?numDaysBack=\" + str(days_back + 1)\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    request = requests.get(url)\n",
    "    soup = BeautifulSoup(request.content, 'xml')\n",
    "#     print(\"Passed the inexplicable bug\") ... I swear I changed nothing and it went away.\n",
    "        \n",
    "    links = [link.text for link in soup.find_all('link')[2:]] # First two links are not articles\n",
    "    titles = [title.text for title in soup.find_all('title')[2:]] # First two titles are not articles\n",
    "    dates = [date.text for date in soup.find_all('pubDate')]\n",
    "    \n",
    "    for link, title, date in zip(links, titles, dates):\n",
    "        \n",
    "        ar_date = datetime.strptime(date, \"%a, %d %b %Y %H:%M:%S -0600\")\n",
    "        \n",
    "        ar_page_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "        ar_main = ar_page_soup.find('main')\n",
    "        body_soup = BeautifulSoup(str(ar_main), 'html.parser')\n",
    "        body = body_soup.find('div', class_=\"goa-grid-100-100-100\").text\n",
    "        \n",
    "        row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "        rows.append(row)\n",
    "                \n",
    "    return pd.DataFrame(rows, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed the inexplicable bug\n",
      "CPU times: user 30.5 s, sys: 1.66 s, total: 32.2 s\n",
      "Wall time: 7min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = load_alberta()\n",
    "df.to_csv('sources/alberta.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
