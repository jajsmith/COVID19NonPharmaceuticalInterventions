{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Source Retrieval\n",
    "\n",
    "Done:\n",
    "- Ontario\n",
    "- Manitoba\n",
    "- British Columbia\n",
    "\n",
    "TODO:\n",
    "- Nova Scotia\n",
    "- Northwest Territories\n",
    "- Nunavut\n",
    "- Yukon\n",
    "- Alberta\n",
    "- Saskatchewan\n",
    "- Prince Edward Island\n",
    "- Quebec\n",
    "- New Brunswick\n",
    "- Newfoundland and Labrador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.6/dist-packages (from beautifulsoup4->bs4) (2.0.1)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: feedparser in /usr/local/lib/python3.6/dist-packages (5.2.1)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import feedparser\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020%2F06%2F04 04:08:12.378860\n"
     ]
    }
   ],
   "source": [
    "today = date.today()\n",
    "today_str = str(today).replace('-', '%2F')\n",
    "\n",
    "print(today_str)\n",
    "country = 'Canada'\n",
    "src_cat = 'Government Website'\n",
    "columns = ['start_date', 'country', 'region', 'subregion', 'source_url', 'source_category', 'source_title', 'source_full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontario\n",
    "\n",
    "Since Ontario shows the most recent news on the first page, the range\n",
    "will need to continue to be expanded to capture all posts. Generally ~4 pages capture ~2 weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching page  1\n",
      "Searching page  2\n",
      "Searching page  3\n",
      "Searching page  4\n",
      "Searching page  5\n",
      "Searching page  6\n",
      "Searching page  7\n",
      "Searching page  8\n",
      "Searching page  9\n",
      "Searching page  10\n",
      "Searching page  11\n",
      "Searching page  12\n",
      "Searching page  13\n",
      "Searching page  14\n",
      "Searching page  15\n",
      "Searching page  16\n",
      "Searching page  17\n",
      "Searching page  18\n",
      "Searching page  19\n",
      "Searching page  20\n",
      "Searching page  21\n",
      "Searching page  22\n",
      "Searching page  23\n",
      "Searching page  24\n",
      "Searching page  25\n",
      "Searching page  26\n",
      "Searching page  27\n",
      "Searching page  28\n",
      "Searching page  29\n",
      "Searching page  30\n",
      "Searching page  31\n",
      "Searching page  32\n",
      "Searching page  33\n",
      "Searching page  34\n",
      "Searching page  35\n",
      "Searching page  36\n",
      "Searching page  37\n",
      "Searching page  38\n",
      "Searching page  39\n",
      "Searching page  40\n",
      "Searching page  41\n",
      "Searching page  42\n",
      "Searching page  43\n",
      "Searching page  44\n",
      "Searching page  45\n",
      "Searching page  46\n",
      "Searching page  47\n",
      "Searching page  48\n",
      "Searching page  49\n",
      "Searching page  50\n",
      "Searching page  51\n",
      "Searching page  52\n",
      "Searching page  53\n",
      "Searching page  54\n",
      "Searching page  55\n",
      "Searching page  56\n",
      "Searching page  57\n",
      "Searching page  58\n",
      "Searching page  59\n",
      "Searching page  60\n",
      "Searching page  61\n",
      "Searching page  62\n",
      "Searching page  63\n",
      "Searching page  64\n",
      "Searching page  65\n",
      "Searching page  66\n",
      "Searching page  67\n",
      "Searching page  68\n",
      "Searching page  69\n",
      "Searching page  70\n",
      "Searching page  71\n",
      "Searching page  72\n",
      "Searching page  73\n",
      "Searching page  74\n",
      "Searching page  75\n",
      "Searching page  76\n",
      "Searching page  77\n",
      "Searching page  78\n",
      "Searching page  79\n",
      "Searching page  80\n",
      "Searching page  81\n",
      "Searching page  82\n",
      "Searching page  83\n",
      "Searching page  84\n",
      "Searching page  85\n",
      "Searching page  86\n",
      "Searching page  87\n",
      "Searching page  88\n",
      "Searching page  89\n",
      "Searching page  90\n",
      "Searching page  91\n",
      "No articles found.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "base_url = 'https://news.ontario.ca/en/search?content_type=all&utf8=%E2%9C%93&date_range_end=' + today_str + '&date_range_start=2020%2F01%2F01&date_select=desc&page='\n",
    "targets = [base_url + str(i) for i in range(1,4)]\n",
    "\n",
    "region = 'Ontario'\n",
    "subregion = ''\n",
    "\n",
    "# Specific structure for news.contario.ca/archive\n",
    "links = []\n",
    "page = 1\n",
    "while True:\n",
    "    print('Searching page ', page)\n",
    "    target = base_url + str(page)\n",
    "    \n",
    "    response = requests.get(target)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.findAll('article')\n",
    "    \n",
    "    if len(articles) == 0:\n",
    "        print('No articles found.')\n",
    "        break\n",
    "\n",
    "    for article in articles:\n",
    "        smallersoup = BeautifulSoup(str(article), \"html.parser\")\n",
    "        link = smallersoup.findAll('a')[0]['href']\n",
    "        title = smallersoup.findAll('a')[0].string\n",
    "        pub_date = datetime(smallersoup.time.string)\n",
    "\n",
    "        response = requests.get(link)\n",
    "        linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        full_text = linksoup.article.text\n",
    "\n",
    "        row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "        links.append(row)\n",
    "    \n",
    "    page += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(links, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892, 8)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['start_date'] = pd.to_datetime(df['start_date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sources/ontario.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manitoba\n",
    "\n",
    "Retrieve all news releases in 2020 for the Province of Manitoba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://news.gov.mb.ca'\n",
    "targets = [url_base + '/news/index.html?month=' + str(i) + '&year=2020&day=01&bgnG=GO&d=' for i in range(1,12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.gov.mb.ca/news/index.html?month=1&year=2020&day=01&bgnG=GO&d=\n",
      "https://news.gov.mb.ca/news/index.html?month=2&year=2020&day=01&bgnG=GO&d=\n",
      "https://news.gov.mb.ca/news/index.html?month=3&year=2020&day=01&bgnG=GO&d=\n",
      "https://news.gov.mb.ca/news/index.html?month=4&year=2020&day=01&bgnG=GO&d=\n",
      "https://news.gov.mb.ca/news/index.html?month=5&year=2020&day=01&bgnG=GO&d=\n",
      "https://news.gov.mb.ca/news/index.html?month=6&year=2020&day=01&bgnG=GO&d=\n",
      "https://news.gov.mb.ca/news/index.html?month=7&year=2020&day=01&bgnG=GO&d=\n",
      "https://news.gov.mb.ca/news/index.html?month=8&year=2020&day=01&bgnG=GO&d=\n",
      "https://news.gov.mb.ca/news/index.html?month=9&year=2020&day=01&bgnG=GO&d=\n",
      "https://news.gov.mb.ca/news/index.html?month=10&year=2020&day=01&bgnG=GO&d=\n",
      "https://news.gov.mb.ca/news/index.html?month=11&year=2020&day=01&bgnG=GO&d=\n",
      "CPU times: user 18.3 s, sys: 550 ms, total: 18.8 s\n",
      "Wall time: 8min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "region = 'Manitoba'\n",
    "subregion = ''\n",
    "\n",
    "links = []\n",
    "for target in targets:\n",
    "    print(target)\n",
    "    if target.startswith(url_base): #manitoba\n",
    "        response = requests.get(target)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        items = soup.findAll(\"div\", {\"class\": \"maincontent\"})\n",
    "        smallersoup = BeautifulSoup(str(items), \"html.parser\")\n",
    "        for article in smallersoup.findAll('h2'):\n",
    "            a = article.a\n",
    "            relative_link = a['href']\n",
    "            link = url_base + relative_link.split('..')[-1]\n",
    "            title = a.string\n",
    "        \n",
    "            response = requests.get(link)\n",
    "            linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            date_text = linksoup.findAll(\"span\", {\"class\": \"article_date\"})[0].string\n",
    "            date = pd.to_datetime(date_text, format='%B %d, %Y')\n",
    "            pub_date = date.strftime('%m/%d/%Y')\n",
    "            \n",
    "            full_text = linksoup.findAll(\"div\", {\"class\": \"\"})[0].text\n",
    "\n",
    "            \n",
    "            row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "            links.append(row)\n",
    "            \n",
    "            # Get this link and copy full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(355, 7)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(links, columns=columns)\n",
    "df.to_csv('sources/manitoba.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## British Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    'https://news.gov.bc.ca/Search?fromDate=2020%2F01%2F01&toDate=2020%2F05%2F26&q='\n",
    "region = 'British Columbia'\n",
    "subregion = ''\n",
    "\n",
    "query_url = 'https://news.gov.bc.ca/Search?FromDate=01/01/2020&Page='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page  1\n",
      "Page  2\n",
      "Page  3\n",
      "Page  4\n",
      "Page  5\n",
      "Page  6\n",
      "Page  7\n",
      "Page  8\n",
      "Page  9\n",
      "Page  10\n",
      "Page  11\n",
      "Page  12\n",
      "Page  13\n",
      "Page  14\n",
      "Page  15\n",
      "Page  16\n",
      "Page  17\n",
      "Page  18\n",
      "Page  19\n",
      "Couldn't retrieve full text for link:  https://news.gov.bc.ca/factsheets/bc-takes-steps-to-support-people-businesses-during-covid-19-pandemic\n",
      "Page  20\n",
      "Page  21\n",
      "Page  22\n",
      "Page  23\n",
      "Page  24\n",
      "Page  25\n",
      "Page  26\n",
      "Page  27\n",
      "Page  28\n",
      "Page  29\n",
      "Page  30\n",
      "Page  31\n",
      "Page  32\n",
      "Page  33\n",
      "Page  34\n",
      "Page  35\n",
      "Page  36\n",
      "Page  37\n",
      "Page  38\n",
      "Page  39\n",
      "Page  40\n",
      "Page  41\n",
      "Page  42\n",
      "Page  43\n",
      "Page  44\n",
      "Page  45\n",
      "Page  46\n",
      "Page  47\n",
      "Page  48\n",
      "Page  49\n",
      "Page  50\n",
      "Page  51\n",
      "Page  52\n",
      "Page  53\n",
      "Page  54\n",
      "Page  55\n",
      "Page  56\n",
      "Page  57\n",
      "Page  58\n",
      "Page  59\n",
      "Page  60\n",
      "Page  61\n",
      "Page  62\n",
      "Page  63\n",
      "Page  64\n",
      "Page  65\n",
      "Page  66\n",
      "Page  67\n",
      "Page  68\n",
      "Page  69\n",
      "Page  70\n",
      "Page  71\n",
      "Page  72\n",
      "Page  73\n",
      "Page  74\n",
      "Page  75\n"
     ]
    }
   ],
   "source": [
    "page = 1\n",
    "while True:\n",
    "    print(\"Page \", page)\n",
    "    target = query_url + str(page)\n",
    "    response = requests.get(target)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    items = soup.findAll(\"div\", {\"class\": \"article\"})\n",
    "    \n",
    "    if not items:\n",
    "        # No articles found\n",
    "        break\n",
    "    \n",
    "    for article in items:\n",
    "        smallersoup = BeautifulSoup(str(article), \"html.parser\")\n",
    "\n",
    "        #for article in smallersoup.findAll('div'):\n",
    "\n",
    "        title = smallersoup.a.string\n",
    "\n",
    "        date_text = smallersoup.findAll(\"div\", {\"class\" : \"item-date\"})[0].string\n",
    "        date = pd.to_datetime(date_text)\n",
    "        pub_date = date.strftime('%m/%d/%Y')\n",
    "\n",
    "        link = smallersoup.a['href']\n",
    "\n",
    "        response = requests.get(link)\n",
    "        linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        get_article = linksoup.findAll(\"article\")\n",
    "        if get_article:\n",
    "            full_text = get_article[0].text\n",
    "        else:\n",
    "            print(\"Couldn't retrieve full text for link: \", link)\n",
    "            full_text = \"\"\n",
    "\n",
    "        row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "        links.append(row)\n",
    "\n",
    "    page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(739, 8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(links, columns=columns)\n",
    "df.to_csv('sources/britishcolumbia.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
