{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Source Retrieval\n",
    "\n",
    "Done:\n",
    "- Ontario\n",
    "- Manitoba\n",
    "- British Columbia\n",
    "- New Brunswick\n",
    "- Nova Scotia\n",
    "- Northwest Territories\n",
    "- Saskatchewan\n",
    "- Nunavut\n",
    "- Yukon\n",
    "- Prince Edward Island\n",
    "- Quebec\n",
    "- Alberta\n",
    "\n",
    "TODO:\n",
    "- Newfoundland and Labrador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from bs4) (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from beautifulsoup4->bs4) (2.0.1)\n",
      "Requirement already satisfied: feedparser in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (5.2.1)\n",
      "Requirement already satisfied: lxml in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (4.5.2)\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install bs4\n",
    "!python3 -m pip install feedparser\n",
    "!python3 -m pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import feedparser\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'Canada'\n",
    "src_cat = 'Government Website'\n",
    "columns = ['start_date', 'country', 'region', 'subregion', 'source_url', 'source_category', 'source_title', 'source_full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontario\n",
    "\n",
    "Since Ontario shows the most recent news on the first page, the range\n",
    "will need to continue to be expanded to capture all posts. Generally ~4 pages capture ~2 weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_ontario(since=datetime(2020, 1, 1), verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `since` \n",
    "            datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \"\"\"\n",
    "\n",
    "    today = date.today()\n",
    "    today_str = str(today).replace('-', '%2F')\n",
    "\n",
    "    base_url = 'https://news.ontario.ca/en/search?content_type=all&utf8=%E2%9C%93&date_range_end=' + today_str + '&date_range_start=2020%2F01%2F01&date_select=desc&page='\n",
    "#     targets = [base_url + str(i) for i in range(1,4)]\n",
    "\n",
    "    region = 'Ontario'\n",
    "    subregion = ''\n",
    "\n",
    "    # Specific structure for news.contario.ca/archive\n",
    "    rows = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        if verbose: print('Searching page ', page)\n",
    "        target = base_url + str(page)\n",
    "\n",
    "        response = requests.get(target)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        articles = soup.findAll('article')\n",
    "\n",
    "        if len(articles) == 0:\n",
    "            if verbose: print('No articles found.')\n",
    "            return pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "        for article in articles:\n",
    "            smallersoup = BeautifulSoup(str(article), \"html.parser\")\n",
    "            link = smallersoup.findAll('a')[0]['href']\n",
    "            title = smallersoup.findAll('a')[0].string\n",
    "            pub_date = datetime.strptime(smallersoup.time.string.replace('.', ''), \"%B %d, %Y %I:%M %p\")\n",
    "            \n",
    "            if pub_date < since:\n",
    "                return pd.DataFrame(rows, columns=columns)\n",
    "            \n",
    "            response = requests.get(link)\n",
    "            linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            full_text = linksoup.article.text\n",
    "\n",
    "            row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "            rows.append(row)\n",
    "\n",
    "        page += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manitoba\n",
    "\n",
    "Retrieve all news releases in 2020 for the Province of Manitoba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_manitoba(since=datetime(2020, 1, 1), verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `since` \n",
    "            datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "    \n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \"\"\"\n",
    "\n",
    "    url_base = 'https://news.gov.mb.ca'\n",
    "    targets = [url_base + '/news/index.html?month=' + str(i) + '&year=2020&day=01&bgnG=GO&d=' for i in range(12,1,-1)] # prevents stopping early\n",
    "\n",
    "    region = 'Manitoba'\n",
    "    subregion = ''\n",
    "\n",
    "    rows = []\n",
    "    for target in targets:\n",
    "        if verbose: print(target)\n",
    "        if target.startswith(url_base): #manitoba\n",
    "            response = requests.get(target)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            items = soup.findAll(\"div\", {\"class\": \"maincontent\"})\n",
    "            smallersoup = BeautifulSoup(str(items), \"html.parser\")\n",
    "            for article in smallersoup.findAll('h2'):\n",
    "                a = article.a\n",
    "                relative_link = a['href']\n",
    "                link = url_base + relative_link.split('..')[-1]\n",
    "                title = a.string\n",
    "\n",
    "                response = requests.get(link)\n",
    "                linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                date_text = linksoup.findAll(\"span\", {\"class\": \"article_date\"})[0].string\n",
    "                pub_date = datetime.strptime(date_text, '%B %d, %Y') # January 31, 2020\n",
    "                \n",
    "                if pub_date < since:\n",
    "                    return pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "                full_text = linksoup.findAll(\"div\", {\"class\": \"\"})[0].text\n",
    "\n",
    "\n",
    "                row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "                rows.append(row)\n",
    "\n",
    "                # Get this link and copy full text\n",
    "    return pd.DataFrame(rows, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# British Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_british_columbia(since=datetime(2020, 1, 1), verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `since` \n",
    "            datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'British Columbia'\n",
    "    subregion = ''\n",
    "\n",
    "    query_url = 'https://news.gov.bc.ca/Search?FromDate=01/01/2020&Page='\n",
    "    rows = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        if verbose: print(\"Page \", page)\n",
    "        target = query_url + str(page)\n",
    "        response = requests.get(target)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        items = soup.findAll(\"div\", {\"class\": \"article\"})\n",
    "\n",
    "        if not items:\n",
    "            return pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "        for article in items:\n",
    "            smallersoup = BeautifulSoup(str(article), \"html.parser\")\n",
    "\n",
    "            #for article in smallersoup.findAll('div'):\n",
    "\n",
    "            title = smallersoup.a.string\n",
    "\n",
    "            date_text = smallersoup.findAll(\"div\", {\"class\" : \"item-date\"})[0].string\n",
    "            pub_date = datetime.strptime(date_text, '%A, %B %d, %Y %I:%M %p') # Friday, July 10, 2020 12:30 PM\n",
    "            \n",
    "            if pub_date < since:\n",
    "                return pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "            link = smallersoup.a['href']\n",
    "\n",
    "            response = requests.get(link)\n",
    "            linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            get_article = linksoup.findAll(\"article\")\n",
    "            if get_article:\n",
    "                full_text = get_article[0].text\n",
    "            else:\n",
    "                if verbose: print(\"Couldn't retrieve full text for link: \", link)\n",
    "                full_text = \"\"\n",
    "\n",
    "            row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "            rows.append(row)\n",
    "\n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Brunswick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_new_brunswick(since=datetime(2020, 1, 1), verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `since` \n",
    "            datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    \n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \"\"\"\n",
    "    region = 'New Brunswick'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www2.gnb.ca/\"\n",
    "    url = url_base + \"content/gnb/en/news/recent_news.html?mainContent_par_newslist_start=\"\n",
    "    start = 0\n",
    "    rows = []\n",
    "    \n",
    "    while True:\n",
    "        if verbose: print(\"Page {}\".format(str(start // 25 + 1)))\n",
    "        response = requests.get(url + str(start))\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        article_div = soup.find('div', class_=\"none padded\")\n",
    "        article_soup = BeautifulSoup(str(article_div), 'html.parser')\n",
    "        articles = article_soup.find_all('li')\n",
    "\n",
    "        for article in articles:\n",
    "            small_soup = BeautifulSoup(str(article), 'html.parser')\n",
    "            ar_date_str = small_soup.find('span', class_=\"post_date\")\n",
    "            \n",
    "            if ar_date_str: # ensure list entry corresponds to dated article\n",
    "                # Date\n",
    "                ar_date = datetime.strptime(ar_date_str.text, \"%d %B %Y\")\n",
    "                \n",
    "                if ar_date < since: # only collect data after specified date\n",
    "                    if verbose: print(\"Stopping search at date {}\".format(ar_date))\n",
    "                    return pd.DataFrame(rows, columns=columns) \n",
    "                \n",
    "                a = article.a\n",
    "                # Title\n",
    "                title = a.text\n",
    "                # Body\n",
    "                relative_link = a['href']\n",
    "                link = url_base + relative_link\n",
    "                article_page = requests.get(link)\n",
    "                body_soup = BeautifulSoup(article_page.content, 'html.parser')\n",
    "                body = body_soup.find('div', class_=\"articleBody\").text\n",
    "                \n",
    "                row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "                rows.append(row)\n",
    "#                 print(\"{}: {}\\n\".format(ar_date, title))\n",
    "                \n",
    "\n",
    "        start += 25 # articles per page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nova Scotia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_nova_scotia(since=datetime(2020, 1, 1), verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `since` \n",
    "            datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of Nova Scotia. \n",
    "    \"\"\"\n",
    "    region = 'Nova Scotia'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://novascotia.ca/news\"\n",
    "    page = 1\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"/search/?page=\" + str(page)\n",
    "        if verbose: print(\"Searching page {}\".format(page))\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        titles = soup.find_all('dt', class_=\"RelTitle\")\n",
    "        summaries = soup.find_all('dd', class_=\"RelSummary\")\n",
    "        \n",
    "        for title, summary in zip(titles, summaries):\n",
    "            \n",
    "            if title['lang'] == \"fr\": continue\n",
    "                        \n",
    "            ar_date = datetime.strptime(summary.time.text, \"%B %d, %Y - %I:%M %p\")\n",
    "            \n",
    "            if ar_date < since:\n",
    "                if verbose: print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(rows, columns=columns)\n",
    "            \n",
    "            relative_link = title.a['href'].split('..', 1)[1]\n",
    "            link = url_base + relative_link\n",
    "            \n",
    "            ar_response = requests.get(link)\n",
    "            ar_soup = BeautifulSoup(ar_response.content, 'html.parser')\n",
    "            body = ar_soup.find('div', {'id' : 'releaseBody'}).text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title.text, body]\n",
    "            rows.append(row)\n",
    "\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Northwest Territories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_northwest_territories(since=datetime(2020, 1, 1), verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `since` \n",
    "            datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of the Northwest Territories.    \n",
    "    \"\"\"\n",
    "    region = 'Northwest Territories'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www.gov.nt.ca/\"\n",
    "    page = 0\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"en/newsroom?page=\" + str(page)\n",
    "        if verbose: print(\"Searching page {}\".format(page + 1))\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        ar_boxes = soup.find_all('div', class_ = re.compile('views-row')) # regex accounts for inconsistent `div` class names\n",
    "        \n",
    "        for box in ar_boxes:\n",
    "            boxed_soup = BeautifulSoup(str(box), 'html.parser') # parse each div\n",
    "            date_str = boxed_soup.find('span').text\n",
    "            ar_date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                if verbose: print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(rows, columns=columns)\n",
    "            \n",
    "            title_a = boxed_soup.find('a')\n",
    "            title = title_a.text\n",
    "            relative_link = title_a['href']\n",
    "            \n",
    "            link = url_base + relative_link\n",
    "            ar_res = requests.get(link)\n",
    "            ar_soup = BeautifulSoup(ar_res.content, 'html.parser')\n",
    "            body = ar_soup.find('div', class_ = \"field-item even\").text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            rows.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saskatchewan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_saskatchewan(since=datetime(2020, 1, 1), verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `since` \n",
    "            datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of Saskatchewan.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Saskatchewan'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www.saskatchewan.ca/government/news-and-media?page=\"\n",
    "    page = 1\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + str(page)\n",
    "        if verbose: print(\"Searching page {}\".format(page))\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        article_list = soup.find('ul', class_=\"results\")\n",
    "        article_soup = BeautifulSoup(str(article_list), 'html.parser')\n",
    "        list_items = article_soup.find_all('li')\n",
    "        \n",
    "        for item in list_items:\n",
    "            \n",
    "            date_str = item.time['datetime']\n",
    "            ar_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                if verbose: print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(rows, columns=columns)\n",
    "            \n",
    "            title = item.a.text\n",
    "            link = item.a['href']\n",
    "            \n",
    "            body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "            body = body_soup.find('section', class_=\"general-content\").text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            rows.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nunavut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_nunavut(since=datetime(2020, 1, 1), verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `since` \n",
    "            datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of Nunavut.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Nunavut'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://gov.nu.ca\"\n",
    "    page = 0\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"/news?page=\" + str(page)\n",
    "        if verbose: print(\"Searching page {}\".format(page + 1))\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        main_section = soup.find('section', {\"id\" : \"block-system-main\"})\n",
    "        main_section_soup = BeautifulSoup(str(main_section), 'html.parser')\n",
    "        \n",
    "        divs = main_section_soup.find_all('div', re.compile('views-row(.*)'))\n",
    "        \n",
    "        for div in divs:\n",
    "            \n",
    "            div_soup = BeautifulSoup(str(div), 'html.parser')\n",
    "            date_str = div_soup.find('span', class_=\"date-display-single\").text\n",
    "            ar_date = datetime.strptime(date_str, \"%d %B %Y\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                if verbose: print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(rows, columns=columns)\n",
    "            \n",
    "            a = div_soup.find('a')\n",
    "            title = a.text\n",
    "            link = url_base + a['href']\n",
    "                        \n",
    "            body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "            body = body_soup.find('div', class_=\"region region-content\").text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            rows.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yukon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_yukon(since=datetime(2020, 1, 1), verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `since` \n",
    "            datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of the Yukon.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Yukon'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://yukon.ca\"\n",
    "    page = 0\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"/news?page=\" + str(page)\n",
    "        if verbose: print(\"Searching page {}\".format(page + 1))\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        main_div = soup.find('div', class_ = \"view-content\")\n",
    "        main_div_soup = BeautifulSoup(str(main_div), 'html.parser')\n",
    "        \n",
    "        divs = main_div_soup.find_all('div', re.compile('views-row(.*)'))\n",
    "        \n",
    "        for div in divs:\n",
    "            \n",
    "            div_soup = BeautifulSoup(str(div), 'html.parser')\n",
    "            date_str = div_soup.find('small').text\n",
    "            ar_date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                if verbose: print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(rows, columns=columns)\n",
    "            \n",
    "            a = div_soup.find('a')\n",
    "            title = a.text\n",
    "            link = url_base + a['href']\n",
    "                        \n",
    "            body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "            body = body_soup.find('div', class_=\"region region-content\").text\n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            rows.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prince Edward Island"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_pei(since=datetime(2020, 1, 1), verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `since` \n",
    "            datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of Prince Edward Island.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Prince Edward Island'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www.princeedwardisland.ca\"\n",
    "    page = 0\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"/news?page=\" + str(page)\n",
    "        if verbose: print(\"Searching page {}\".format(page + 1))\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        divs = soup.find_all('div', class_=\"right content views-fieldset\")\n",
    "        \n",
    "        for div in divs:\n",
    "                        \n",
    "            div_soup = BeautifulSoup(str(div), 'html.parser')\n",
    "            date_str = div_soup.find('div', class_=\"date\").text\n",
    "            ar_date = datetime.strptime(date_str, \"%A, %B %d, %Y\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                if verbose: print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(rows, columns=columns)\n",
    "            \n",
    "            a = div_soup.find('a')\n",
    "            title = a.text\n",
    "            link = url_base + a['href']\n",
    "            \n",
    "            body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "            body = body_soup.find('div', class_=\"maincontentmain\").text\n",
    "                        \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            rows.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_alberta(since=datetime(2020, 1, 1), verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `since` \n",
    "            datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of Alberta.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Alberta'\n",
    "    sub_region = ''\n",
    "    \n",
    "    days_back = (datetime.today() - since).days\n",
    "    url = \"https://www.alberta.ca/NewsRoom/newsroom.cfm?numDaysBack=\" + str(days_back + 1)\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'xml')\n",
    "        \n",
    "    links = [link.text for link in soup.find_all('link')[2:]] # First two links are not articles\n",
    "    titles = [title.text for title in soup.find_all('title')[2:]] # First two titles are not articles\n",
    "    dates = [date.text for date in soup.find_all('pubDate')]\n",
    "    \n",
    "    for link, title, date in zip(links, titles, dates):\n",
    "        \n",
    "        ar_date = datetime.strptime(date, \"%a, %d %b %Y %H:%M:%S -0600\")\n",
    "        \n",
    "        ar_page_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "        ar_main = ar_page_soup.find('main')\n",
    "        body_soup = BeautifulSoup(str(ar_main), 'html.parser')\n",
    "        body = body_soup.find('div', class_=\"goa-grid-100-100-100\").text\n",
    "        \n",
    "        row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "        rows.append(row)\n",
    "                \n",
    "    return pd.DataFrame(rows, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quebec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_quebec(since=datetime(2020, 1, 1), verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `since` \n",
    "            datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of Quebec.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Quebec'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"http://www.fil-information.gouv.qc.ca/Pages/Articles.aspx?lang=en&Page=\"\n",
    "    page = 1\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + str(page)\n",
    "        \n",
    "        if verbose: print(\"Searching page {}\".format(page))\n",
    "        \n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "        sections = soup.find_all('section', {\"id\" : \"articles\"})\n",
    "        \n",
    "        for section in sections:\n",
    "            date_str = section.time['datetime']\n",
    "            ar_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "            \n",
    "            if ar_date < since:\n",
    "                if verbose: print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(rows, columns=columns)\n",
    "            \n",
    "            for a in section.find_all('a'):\n",
    "            \n",
    "                link = a['href']\n",
    "                title = a.text.replace('\\r', '')\n",
    "                title = title.replace('\\n', '')\n",
    "\n",
    "                body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "                body = body_soup.find('div', class_=\"article\").text\n",
    "\n",
    "                row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "                rows.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update CSVs\n",
    "\n",
    "These functions allow for the retrieval of only recent releases not currently stored in the CSVs. Needless to say, scraping all the way back to January 1st takes a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ontario(verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of Ontario.\n",
    "    \"\"\"\n",
    "    ontario = pd.read_csv('sources/ontario.csv')\n",
    "    ontario = ontario.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    ontario[\"start_date\"] = pd.to_datetime(ontario[\"start_date\"])\n",
    "    \n",
    "    largest_date = ontario[\"start_date\"].max()        \n",
    "    new_additions = _load_ontario(since=largest_date, verbose=verbose)  \n",
    "        \n",
    "    df = new_additions.append(ontario).drop_duplicates(['source_full_text', 'source_url'])\n",
    "    df.to_csv('sources/ontario.csv')\n",
    "    return df\n",
    "\n",
    "def load_quebec(verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of Quebec.\n",
    "    \"\"\"\n",
    "    quebec = pd.read_csv('sources/quebec.csv')\n",
    "    quebec = quebec.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    quebec[\"start_date\"] = pd.to_datetime(quebec[\"start_date\"])\n",
    "    \n",
    "    largest_date = quebec[\"start_date\"].max()        \n",
    "    new_additions = _load_quebec(since=largest_date, verbose=verbose)  \n",
    "        \n",
    "    df = new_additions.append(quebec).drop_duplicates(['source_full_text', 'source_url'])\n",
    "    df.to_csv('sources/quebec.csv')\n",
    "    return df\n",
    "\n",
    "def load_northwest_territories(verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of the Northwest Territories.\n",
    "    \"\"\"\n",
    "    northwest_territories = pd.read_csv('sources/northwestterritories.csv')\n",
    "    northwest_territories = northwest_territories.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    northwest_territories[\"start_date\"] = pd.to_datetime(northwest_territories[\"start_date\"])\n",
    "    \n",
    "    largest_date = northwest_territories[\"start_date\"].max()        \n",
    "    new_additions = _load_northwest_territories(since=largest_date, verbose=verbose)  \n",
    "        \n",
    "    df = new_additions.append(northwest_territories).drop_duplicates(['source_full_text', 'source_url'])\n",
    "    df.to_csv('sources/northwestterritories.csv')\n",
    "    return df\n",
    "\n",
    "def load_yukon(verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of the Yukon.\n",
    "    \"\"\"\n",
    "    yukon = pd.read_csv('sources/yukon.csv')\n",
    "    yukon = yukon.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    yukon[\"start_date\"] = pd.to_datetime(yukon[\"start_date\"])\n",
    "    \n",
    "    largest_date = yukon[\"start_date\"].max()      \n",
    "        \n",
    "    new_additions = _load_yukon(since=largest_date, verbose=verbose)  \n",
    "    \n",
    "    df = new_additions.append(yukon).drop_duplicates(['source_full_text', 'source_url'])\n",
    "    df.to_csv('sources/yukon.csv')\n",
    "    return df\n",
    "\n",
    "def load_new_brunswick(verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \"\"\"\n",
    "    new_brunswick = pd.read_csv('sources/newbrunswick.csv')\n",
    "    new_brunswick = new_brunswick.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    new_brunswick[\"start_date\"] = pd.to_datetime(new_brunswick[\"start_date\"])\n",
    "    \n",
    "    largest_date = new_brunswick[\"start_date\"].max()        \n",
    "    new_additions = _load_new_brunswick(since=largest_date, verbose=verbose)  \n",
    "        \n",
    "    df = new_additions.append(new_brunswick).drop_duplicates(['source_full_text', 'source_url'])\n",
    "    df.to_csv('sources/newbrunswick.csv')\n",
    "    return df\n",
    "\n",
    "def load_nova_scotia(verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of Nova Scotia.\n",
    "    \"\"\"\n",
    "    nova_scotia = pd.read_csv('sources/novascotia.csv')\n",
    "    nova_scotia = nova_scotia.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    nova_scotia[\"start_date\"] = pd.to_datetime(nova_scotia[\"start_date\"])\n",
    "    \n",
    "    largest_date = nova_scotia[\"start_date\"].max()        \n",
    "    new_additions = _load_nova_scotia(since=largest_date, verbose=verbose)  \n",
    "        \n",
    "    df = new_additions.append(nova_scotia).drop_duplicates(['source_full_text', 'source_url'])\n",
    "    df.to_csv('sources/novascotia.csv')\n",
    "    return df\n",
    "\n",
    "def load_alberta(verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of Alberta.\n",
    "    \"\"\"\n",
    "    alberta = pd.read_csv('sources/alberta.csv')\n",
    "    alberta = alberta.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    alberta[\"start_date\"] = pd.to_datetime(alberta[\"start_date\"])\n",
    "    \n",
    "    largest_date = alberta[\"start_date\"].max()        \n",
    "    new_additions = _load_alberta(since=largest_date, verbose=verbose)  \n",
    "        \n",
    "    df = new_additions.append(alberta).drop_duplicates(['source_full_text', 'source_url'])\n",
    "    df.to_csv('sources/alberta.csv')\n",
    "    return df\n",
    "\n",
    "def load_manitoba(verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of Manitoba.\n",
    "    \"\"\"\n",
    "    manitoba = pd.read_csv('sources/manitoba.csv')\n",
    "    manitoba = manitoba.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    manitoba[\"start_date\"] = pd.to_datetime(manitoba[\"start_date\"])\n",
    "    \n",
    "    largest_date = manitoba[\"start_date\"].max()        \n",
    "    new_additions = _load_manitoba(since=largest_date, verbose=verbose)  \n",
    "        \n",
    "    df = new_additions.append(manitoba).drop_duplicates(['source_full_text', 'source_url'])\n",
    "    df.to_csv('sources/manitoba.csv')\n",
    "    return df\n",
    "\n",
    "def load_pei(verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of Prince Edward Island.\n",
    "    \"\"\"\n",
    "    pei = pd.read_csv('sources/pei.csv')\n",
    "    pei = pei.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    pei[\"start_date\"] = pd.to_datetime(pei[\"start_date\"])\n",
    "    \n",
    "    largest_date = pei[\"start_date\"].max()        \n",
    "    new_additions = _load_pei(since=largest_date, verbose=verbose)  \n",
    "        \n",
    "    df = new_additions.append(pei).drop_duplicates(['source_full_text', 'source_url'])\n",
    "    df.to_csv('sources/pei.csv')\n",
    "    return df\n",
    "\n",
    "def load_british_columbia(verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of British Columbia.\n",
    "    \"\"\"\n",
    "    british_columbia = pd.read_csv('sources/britishcolumbia.csv')\n",
    "    british_columbia = british_columbia.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    british_columbia[\"start_date\"] = pd.to_datetime(british_columbia[\"start_date\"])\n",
    "    \n",
    "    largest_date = british_columbia[\"start_date\"].max()        \n",
    "    new_additions = _load_british_columbia(since=largest_date, verbose=verbose)  \n",
    "        \n",
    "    df = new_additions.append(british_columbia).drop_duplicates(['source_full_text', 'source_url'])\n",
    "    df.to_csv('sources/britishcolumbia.csv')\n",
    "    return df\n",
    "\n",
    "def load_nunavut(verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of Nunavut.\n",
    "    \"\"\"\n",
    "    nunavut = pd.read_csv('sources/nunavut.csv')\n",
    "    nunavut = nunavut.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    nunavut[\"start_date\"] = pd.to_datetime(nunavut[\"start_date\"])\n",
    "    \n",
    "    largest_date = nunavut[\"start_date\"].max()        \n",
    "    new_additions = _load_nunavut(since=largest_date, verbose=verbose)  \n",
    "        \n",
    "    df = new_additions.append(nunavut).drop_duplicates(['source_full_text', 'source_url'])\n",
    "    df.to_csv('sources/nunavut.csv')\n",
    "    return df\n",
    "\n",
    "def load_saskatchewan(verbose=True):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of Saskatchewan.\n",
    "    \"\"\"\n",
    "    saskatchewan = pd.read_csv('sources/saskatchewan.csv')\n",
    "    saskatchewan = saskatchewan.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    saskatchewan[\"start_date\"] = pd.to_datetime(saskatchewan[\"start_date\"])\n",
    "    \n",
    "    largest_date = saskatchewan[\"start_date\"].max()        \n",
    "    new_additions = _load_saskatchewan(since=largest_date, verbose=verbose)  \n",
    "        \n",
    "    df = new_additions.append(saskatchewan).drop_duplicates(['source_full_text', 'source_url'])\n",
    "    df.to_csv('sources/saskatchewan.csv')\n",
    "    return df\n",
    "\n",
    "def load_provinces(verbose=False):\n",
    "    \"\"\"\n",
    "    Parameters: \n",
    "        - `verbose`\n",
    "            boolean, whether or not the function should print updates (False by default)\n",
    "\n",
    "    Returns: a dictionary mapping the names of provinces and territories to DataFrames containing information about their new releases.\n",
    "    \"\"\"\n",
    "    return {'alberta' : load_alberta(verbose), \n",
    "            'british columbia' : load_british_columbia(verbose), \n",
    "            'manitoba' : load_manitoba(verbose), \n",
    "            'new brunsiwck' : load_new_brunswick(verbose), \n",
    "            'northwest territories' : load_northwest_territories(verbose), \n",
    "            'nova scotia' : load_nova_scotia(verbose), \n",
    "            'nunavut' : load_nunavut(verbose), \n",
    "            'ontario' : load_ontario(verbose), \n",
    "            'pei' : load_pei(verbose), \n",
    "            'quebec' : load_quebec(verbose), \n",
    "            'saskatchewan' : load_saskatchewan(verbose), \n",
    "            'yukon' : load_yukon(verbose)\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pei' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-c096feb073f1>\u001b[0m in \u001b[0;36mload_provinces\u001b[0;34m(verbose)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \"\"\"\n\u001b[1;32m    250\u001b[0m     return {'alberta' : load_alberta(verbose), \n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0;34m'british columbia'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mload_british_columbia\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0;34m'manitoba'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mload_manitoba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;34m'new brunsiwck'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mload_new_brunswick\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-c096feb073f1>\u001b[0m in \u001b[0;36mload_british_columbia\u001b[0;34m(verbose)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mnew_additions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_british_columbia\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlargest_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_additions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpei\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'source_full_text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'source_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sources/britishcolumbia.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pei' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "l = load_provinces()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
