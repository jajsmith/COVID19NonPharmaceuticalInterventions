{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Source Retrieval\n",
    "\n",
    "Done:\n",
    "- Ontario\n",
    "- Manitoba\n",
    "- British Columbia\n",
    "- New Brunswick\n",
    "- Nova Scotia\n",
    "- Northwest Territories\n",
    "- Saskatchewan\n",
    "- Nunavut\n",
    "- Yukon\n",
    "- Prince Edward Island\n",
    "- Quebec\n",
    "- Alberta\n",
    "\n",
    "TODO:\n",
    "- Newfoundland and Labrador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install bs4\n",
    "!python3 -m pip install feedparser\n",
    "!python3 -m pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import feedparser\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = 'Canada'\n",
    "src_cat = 'Government Website'\n",
    "columns = ['start_date', 'country', 'region', 'subregion', 'source_url', 'source_category', 'source_title', 'source_full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ontario\n",
    "\n",
    "Since Ontario shows the most recent news on the first page, the range\n",
    "will need to continue to be expanded to capture all posts. Generally ~4 pages capture ~2 weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ontario(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \"\"\"\n",
    "\n",
    "    today = date.today()\n",
    "    today_str = str(today).replace('-', '%2F')\n",
    "\n",
    "    base_url = 'https://news.ontario.ca/en/search?content_type=all&utf8=%E2%9C%93&date_range_end=' + today_str + '&date_range_start=2020%2F01%2F01&date_select=desc&page='\n",
    "#     targets = [base_url + str(i) for i in range(1,4)]\n",
    "\n",
    "    region = 'Ontario'\n",
    "    subregion = ''\n",
    "\n",
    "    # Specific structure for news.contario.ca/archive\n",
    "    rows = []\n",
    "    page = 1\n",
    "    while True:\n",
    "        print('Searching page ', page)\n",
    "        target = base_url + str(page)\n",
    "\n",
    "        response = requests.get(target)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        articles = soup.findAll('article')\n",
    "\n",
    "        if len(articles) == 0:\n",
    "            print('No articles found.')\n",
    "            return pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "        for article in articles:\n",
    "            smallersoup = BeautifulSoup(str(article), \"html.parser\")\n",
    "            link = smallersoup.findAll('a')[0]['href']\n",
    "            title = smallersoup.findAll('a')[0].string\n",
    "            pub_date = datetime.strptime(smallersoup.time.string.replace('.', ''), \"%B %d, %Y %I:%M %p\")\n",
    "            \n",
    "            if pub_date < since:\n",
    "                return pd.DataFrame(rows, columns=columns)\n",
    "            \n",
    "            response = requests.get(link)\n",
    "            linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            full_text = linksoup.article.text\n",
    "\n",
    "            row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "            rows.append(row)\n",
    "\n",
    "        page += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_ontario()\n",
    "df.shape\n",
    "df.to_csv('sources/ontario.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manitoba\n",
    "\n",
    "Retrieve all news releases in 2020 for the Province of Manitoba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_manitoba(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \"\"\"\n",
    "\n",
    "    url_base = 'https://news.gov.mb.ca'\n",
    "    targets = [url_base + '/news/index.html?month=' + str(i) + '&year=2020&day=01&bgnG=GO&d=' for i in range(1,12)]\n",
    "\n",
    "    region = 'Manitoba'\n",
    "    subregion = ''\n",
    "\n",
    "    rows = []\n",
    "    for target in targets:\n",
    "        print(target)\n",
    "        if target.startswith(url_base): #manitoba\n",
    "            response = requests.get(target)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            items = soup.findAll(\"div\", {\"class\": \"maincontent\"})\n",
    "            smallersoup = BeautifulSoup(str(items), \"html.parser\")\n",
    "            for article in smallersoup.findAll('h2'):\n",
    "                a = article.a\n",
    "                relative_link = a['href']\n",
    "                link = url_base + relative_link.split('..')[-1]\n",
    "                title = a.string\n",
    "\n",
    "                response = requests.get(link)\n",
    "                linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                date_text = linksoup.findAll(\"span\", {\"class\": \"article_date\"})[0].string\n",
    "                date = pd.to_datetime(date_text, format='%B %d, %Y')\n",
    "                pub_date = date.strftime('%m/%d/%Y')\n",
    "\n",
    "                full_text = linksoup.findAll(\"div\", {\"class\": \"\"})[0].text\n",
    "\n",
    "\n",
    "                row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "                rows.append(row)\n",
    "\n",
    "                # Get this link and copy full text\n",
    "    return pd.DataFrame(rows, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_manitoba()\n",
    "df.to_csv('sources/manitoba.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# British Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_british_columbia(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'British Columbia'\n",
    "    subregion = ''\n",
    "\n",
    "    query_url = 'https://news.gov.bc.ca/Search?FromDate=01/01/2020&Page='\n",
    "    rows = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        print(\"Page \", page)\n",
    "        target = query_url + str(page)\n",
    "        response = requests.get(target)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        items = soup.findAll(\"div\", {\"class\": \"article\"})\n",
    "\n",
    "        if not items:\n",
    "            return pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "        for article in items:\n",
    "            smallersoup = BeautifulSoup(str(article), \"html.parser\")\n",
    "\n",
    "            #for article in smallersoup.findAll('div'):\n",
    "\n",
    "            title = smallersoup.a.string\n",
    "\n",
    "            date_text = smallersoup.findAll(\"div\", {\"class\" : \"item-date\"})[0].string\n",
    "            date = pd.to_datetime(date_text)\n",
    "            pub_date = date.strftime('%m/%d/%Y')\n",
    "\n",
    "            link = smallersoup.a['href']\n",
    "\n",
    "            response = requests.get(link)\n",
    "            linksoup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            get_article = linksoup.findAll(\"article\")\n",
    "            if get_article:\n",
    "                full_text = get_article[0].text\n",
    "            else:\n",
    "                print(\"Couldn't retrieve full text for link: \", link)\n",
    "                full_text = \"\"\n",
    "\n",
    "            row = [pub_date, country, region, subregion, link, src_cat, title, full_text]\n",
    "            rows.append(row)\n",
    "\n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_british_columbia()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sources/britishcolumbia.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Brunswick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_new_brunswick(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of New Brunswick.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    region = 'New Brunswick'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www2.gnb.ca/\"\n",
    "    url = url_base + \"content/gnb/en/news/recent_news.html?mainContent_par_newslist_start=\"\n",
    "    start = 0\n",
    "    rows = []\n",
    "    \n",
    "    while True:\n",
    "        print(\"Page {}\".format(str(start // 25 + 1)))\n",
    "        request = requests.get(url + str(start))\n",
    "        soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "\n",
    "        article_div = soup.find('div', class_=\"none padded\")\n",
    "        article_soup = BeautifulSoup(str(article_div), 'html.parser')\n",
    "        articles = article_soup.find_all('li')\n",
    "\n",
    "        for article in articles:\n",
    "            small_soup = BeautifulSoup(str(article), 'html.parser')\n",
    "            ar_date_str = small_soup.find('span', class_=\"post_date\")\n",
    "            \n",
    "            if ar_date_str: # ensure list entry corresponds to dated article\n",
    "                # Date\n",
    "                ar_date = datetime.strptime(ar_date_str.text, \"%d %B %Y\")\n",
    "                \n",
    "                if ar_date < since: # only collect data after specified date\n",
    "                    print(\"Stopping search at date {}\".format(ar_date))\n",
    "                    return pd.DataFrame(rows, columns=columns) \n",
    "                \n",
    "                a = article.a\n",
    "                # Title\n",
    "                title = a.text\n",
    "                # Body\n",
    "                relative_link = a['href']\n",
    "                link = url_base + relative_link\n",
    "                article_page = requests.get(link)\n",
    "                body_soup = BeautifulSoup(article_page.content, 'html.parser')\n",
    "                body = body_soup.find('div', class_=\"articleBody\").text\n",
    "                \n",
    "                row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "                rows.append(row)\n",
    "#                 print(\"{}: {}\\n\".format(ar_date, title))\n",
    "                \n",
    "\n",
    "        start += 25 # articles per page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.DataFrame(load_new_brunswick(), columns=columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sources/newbrunswick.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nova Scotia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nova_scotia(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of Nova Scotia.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \n",
    "    \"\"\"\n",
    "    region = 'Nova Scotia'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://novascotia.ca/news\"\n",
    "    page = 1\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"/search/?page=\" + str(page)\n",
    "        print(\"Searching page {}\".format(page))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        titles = soup.find_all('dt', class_=\"RelTitle\")\n",
    "        summaries = soup.find_all('dd', class_=\"RelSummary\")\n",
    "        \n",
    "        for title, summary in zip(titles, summaries):\n",
    "            \n",
    "            if title['lang'] == \"fr\": continue\n",
    "                        \n",
    "            ar_date = datetime.strptime(summary.time.text, \"%B %d, %Y - %I:%M %p\")\n",
    "            \n",
    "            if ar_date < since:\n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(rows, columns=columns)\n",
    "            \n",
    "            relative_link = title.a['href'].split('..', 1)[1]\n",
    "            link = url_base + relative_link\n",
    "            \n",
    "            ar_request = requests.get(link)\n",
    "            ar_soup = BeautifulSoup(ar_request.content, 'html.parser')\n",
    "            body = ar_soup.find('div', {'id' : 'releaseBody'}).text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title.text, body]\n",
    "            rows.append(row)\n",
    "\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_nova_scotia()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sources/novascotia.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Northwest Territories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_northwest_territories(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of the Northwest Territories.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \n",
    "    \"\"\"\n",
    "    region = 'Northwest Territories'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www.gov.nt.ca/\"\n",
    "    page = 0\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"en/newsroom?page=\" + str(page)\n",
    "        print(\"Searching page {}\".format(page + 1))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        ar_boxes = soup.find_all('div', class_ = re.compile('views-row')) # regex accounts for inconsistent `div` class names\n",
    "        \n",
    "        for box in ar_boxes:\n",
    "            boxed_soup = BeautifulSoup(str(box), 'html.parser') # parse each div\n",
    "            date_str = boxed_soup.find('span').text\n",
    "            ar_date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(rows, columns=columns)\n",
    "            \n",
    "            title_a = boxed_soup.find('a')\n",
    "            title = title_a.text\n",
    "            relative_link = title_a['href']\n",
    "            \n",
    "            link = url_base + relative_link\n",
    "            ar_req = requests.get(link)\n",
    "            ar_soup = BeautifulSoup(ar_req.content, 'html.parser')\n",
    "            body = ar_soup.find('div', class_ = \"field-item even\").text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            rows.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_northwest_territories()\n",
    "df.to_csv('sources/northwestterritories.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saskatchewan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saskatchewan(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of Saskatchewan.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Saskatchewan'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www.saskatchewan.ca/government/news-and-media?page=\"\n",
    "    page = 1\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + str(page)\n",
    "        print(\"Searching page {}\".format(page))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        article_list = soup.find('ul', class_=\"results\")\n",
    "        article_soup = BeautifulSoup(str(article_list), 'html.parser')\n",
    "        list_items = article_soup.find_all('li')\n",
    "        \n",
    "        for item in list_items:\n",
    "            \n",
    "            date_str = item.time['datetime']\n",
    "            ar_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(rows, columns=columns)\n",
    "            \n",
    "            title = item.a.text\n",
    "            link = item.a['href']\n",
    "            \n",
    "            body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "            body = body_soup.find('section', class_=\"general-content\").text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            rows.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_saskatchewan()\n",
    "df.to_csv('sources/saskatchewan.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nunavut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nunavut(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of Nunavut.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Nunavut'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://gov.nu.ca\"\n",
    "    page = 0\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"/news?page=\" + str(page)\n",
    "        print(\"Searching page {}\".format(page + 1))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        main_section = soup.find('section', {\"id\" : \"block-system-main\"})\n",
    "        main_section_soup = BeautifulSoup(str(main_section), 'html.parser')\n",
    "        \n",
    "        divs = main_section_soup.find_all('div', re.compile('views-row(.*)'))\n",
    "        \n",
    "        for div in divs:\n",
    "            \n",
    "            div_soup = BeautifulSoup(str(div), 'html.parser')\n",
    "            date_str = div_soup.find('span', class_=\"date-display-single\").text\n",
    "            ar_date = datetime.strptime(date_str, \"%d %B %Y\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(rows, columns=columns)\n",
    "            \n",
    "            a = div_soup.find('a')\n",
    "            title = a.text\n",
    "            link = url_base + a['href']\n",
    "                        \n",
    "            body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "            body = body_soup.find('div', class_=\"region region-content\").text\n",
    "            \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            rows.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_nunavut()\n",
    "df.to_csv('sources/nunavut.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yukon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yukon(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of the Yukon.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Yukon'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://yukon.ca\"\n",
    "    page = 0\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"/news?page=\" + str(page)\n",
    "        print(\"Searching page {}\".format(page + 1))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        main_div = soup.find('div', class_ = \"view-content\")\n",
    "        main_div_soup = BeautifulSoup(str(main_div), 'html.parser')\n",
    "        \n",
    "        divs = main_div_soup.find_all('div', re.compile('views-row(.*)'))\n",
    "        \n",
    "        for div in divs:\n",
    "            \n",
    "            div_soup = BeautifulSoup(str(div), 'html.parser')\n",
    "            date_str = div_soup.find('small').text\n",
    "            ar_date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(rows, columns=columns)\n",
    "            \n",
    "            a = div_soup.find('a')\n",
    "            title = a.text\n",
    "            link = url_base + a['href']\n",
    "                        \n",
    "            body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "            body = body_soup.find('div', class_=\"region region-content\").text\n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            rows.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_yukon()\n",
    "df.to_csv('sources/yukon.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prince Edward Island"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pei(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of Prince Edward Island.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Prince Edward Island'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"https://www.princeedwardisland.ca\"\n",
    "    page = 0\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + \"/news?page=\" + str(page)\n",
    "        print(\"Searching page {}\".format(page + 1))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "        \n",
    "        divs = soup.find_all('div', class_=\"right content views-fieldset\")\n",
    "        \n",
    "        for div in divs:\n",
    "                        \n",
    "            div_soup = BeautifulSoup(str(div), 'html.parser')\n",
    "            date_str = div_soup.find('div', class_=\"date\").text\n",
    "            ar_date = datetime.strptime(date_str, \"%A, %B %d, %Y\")\n",
    "            \n",
    "            if ar_date < since: \n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(rows, columns=columns)\n",
    "            \n",
    "            a = div_soup.find('a')\n",
    "            title = a.text\n",
    "            link = url_base + a['href']\n",
    "            \n",
    "            body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "            body = body_soup.find('div', class_=\"maincontentmain\").text\n",
    "                        \n",
    "            row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "            rows.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_pei()\n",
    "df.to_csv('sources/pei.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_alberta(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of Alberta.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Alberta'\n",
    "    sub_region = ''\n",
    "    \n",
    "    days_back = (datetime.today() - since).days\n",
    "    url = \"https://www.alberta.ca/NewsRoom/newsroom.cfm?numDaysBack=\" + str(days_back + 1)\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    request = requests.get(url)\n",
    "    soup = BeautifulSoup(request.content, 'xml')\n",
    "#     print(\"Passed the inexplicable bug\") ... I swear I changed nothing and it went away.\n",
    "        \n",
    "    links = [link.text for link in soup.find_all('link')[2:]] # First two links are not articles\n",
    "    titles = [title.text for title in soup.find_all('title')[2:]] # First two titles are not articles\n",
    "    dates = [date.text for date in soup.find_all('pubDate')]\n",
    "    \n",
    "    for link, title, date in zip(links, titles, dates):\n",
    "        \n",
    "        ar_date = datetime.strptime(date, \"%a, %d %b %Y %H:%M:%S -0600\")\n",
    "        \n",
    "        ar_page_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "        ar_main = ar_page_soup.find('main')\n",
    "        body_soup = BeautifulSoup(str(ar_main), 'html.parser')\n",
    "        body = body_soup.find('div', class_=\"goa-grid-100-100-100\").text\n",
    "        \n",
    "        row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "        rows.append(row)\n",
    "                \n",
    "    return pd.DataFrame(rows, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_alberta()\n",
    "df.to_csv('sources/alberta.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quebec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_quebec(since=datetime(2020, 1, 1)):\n",
    "    \"\"\"\n",
    "    Returns: a DataFrame containing news releases from the government of Quebec.\n",
    "    \n",
    "    Parameters: datetime object, the date of the earliest news release to be retrieved. By default, only the releases published since Jan 1 2020 are retrieved.\n",
    "    \"\"\"\n",
    "    \n",
    "    region = 'Quebec'\n",
    "    sub_region = ''\n",
    "    \n",
    "    url_base = \"http://www.fil-information.gouv.qc.ca/Pages/Articles.aspx?lang=en&Page=\"\n",
    "    page = 1\n",
    "    \n",
    "    rows = []\n",
    "    \n",
    "    while True:\n",
    "        url = url_base + str(page)\n",
    "        print(\"Searching page {}\".format(page))\n",
    "        \n",
    "        request = requests.get(url)\n",
    "        soup = BeautifulSoup(request.content, 'html.parser')\n",
    "                \n",
    "        sections = soup.find_all('section', {\"id\" : \"articles\"})\n",
    "        \n",
    "        for section in sections:\n",
    "            date_str = section.time['datetime']\n",
    "            ar_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "            \n",
    "            if ar_date < since:\n",
    "                print(\"Stopping search at date {}\".format(ar_date))\n",
    "                return pd.DataFrame(rows, columns=columns)\n",
    "            \n",
    "            for a in section.find_all('a'):\n",
    "            \n",
    "                link = a['href']\n",
    "                title = a.text.replace('\\r', '')\n",
    "                title = title.replace('\\n', '')\n",
    "\n",
    "                body_soup = BeautifulSoup(requests.get(link).content, 'html.parser')\n",
    "                body = body_soup.find('div', class_=\"article\").text\n",
    "\n",
    "                row = [ar_date, country, region, sub_region, link, src_cat, title, body]\n",
    "                rows.append(row)\n",
    "            \n",
    "        page += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = load_quebec()\n",
    "df.to_csv('sources/quebec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_ontario():\n",
    "    ontario = pd.read_csv('sources/ontario.csv')\n",
    "    ontario = ontario.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    ontario[\"start_date\"] = pd.to_datetime(ontario[\"start_date\"])\n",
    "    \n",
    "    largest_date = ontario[\"start_date\"].max()        \n",
    "    new_additions = load_ontario(since=largest_date)  \n",
    "        \n",
    "    df = ontario.append(new_additions)\n",
    "    df.to_csv('sources/ontario.csv')\n",
    "    return df.drop_duplicates(['source_full_text'], keep=False)\n",
    "\n",
    "def update_quebec():\n",
    "    quebec = pd.read_csv('sources/quebec.csv')\n",
    "    quebec = quebec.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    quebec[\"start_date\"] = pd.to_datetime(quebec[\"start_date\"])\n",
    "    \n",
    "    largest_date = quebec[\"start_date\"].max()        \n",
    "    new_additions = load_quebec(since=largest_date)  \n",
    "        \n",
    "    df = quebec.append(new_additions)\n",
    "    df.to_csv('sources/quebec.csv')\n",
    "    return df.drop_duplicates(['source_full_text'], keep=False)\n",
    "\n",
    "\n",
    "\n",
    "def update_northwest_territories():\n",
    "    northwest_territories = pd.read_csv('sources/northwestterritories.csv')\n",
    "    northwest_territories = northwest_territories.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    northwest_territories[\"start_date\"] = pd.to_datetime(northwest_territories[\"start_date\"])\n",
    "    \n",
    "    largest_date = northwest_territories[\"start_date\"].max()        \n",
    "    new_additions = load_northwest_territories(since=largest_date)  \n",
    "        \n",
    "    df = northwest_territories.append(new_additions)\n",
    "    df.to_csv('sources/northwestterritories.csv')\n",
    "    return df.drop_duplicates(['source_full_text'], keep=False)\n",
    "\n",
    "\n",
    "\n",
    "def update_yukon():\n",
    "    yukon = pd.read_csv('sources/yukon.csv')\n",
    "    yukon = yukon.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    yukon[\"start_date\"] = pd.to_datetime(yukon[\"start_date\"])\n",
    "    \n",
    "    largest_date = yukon[\"start_date\"].max()        \n",
    "    new_additions = load_yukon(since=largest_date)  \n",
    "        \n",
    "    df = yukon.append(new_additions)\n",
    "    df.to_csv('sources/yukon.csv')\n",
    "    return df.drop_duplicates(['source_full_text'], keep=False)\n",
    "\n",
    "\n",
    "\n",
    "def update_new_brunswick():\n",
    "    new_brunswick = pd.read_csv('sources/newbrunswick.csv')\n",
    "    new_brunswick = new_brunswick.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    new_brunswick[\"start_date\"] = pd.to_datetime(new_brunswick[\"start_date\"])\n",
    "    \n",
    "    largest_date = new_brunswick[\"start_date\"].max()        \n",
    "    new_additions = load_new_brunswick(since=largest_date)  \n",
    "        \n",
    "    df = new_brunswick.append(new_additions)\n",
    "    df.to_csv('sources/newbrunswick.csv')\n",
    "    return df.drop_duplicates(['source_full_text'], keep=False)\n",
    "\n",
    "\n",
    "\n",
    "def update_nova_scotia():\n",
    "    nova_scotia = pd.read_csv('sources/novascotia.csv')\n",
    "    nova_scotia = nova_scotia.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    nova_scotia[\"start_date\"] = pd.to_datetime(nova_scotia[\"start_date\"])\n",
    "    \n",
    "    largest_date = nova_scotia[\"start_date\"].max()        \n",
    "    new_additions = load_nova_scotia(since=largest_date)  \n",
    "        \n",
    "    df = nova_scotia.append(new_additions)\n",
    "    df.to_csv('sources/novascotia.csv')\n",
    "    return df.drop_duplicates(['source_full_text'], keep=False)\n",
    "\n",
    "\n",
    "\n",
    "def update_alberta():\n",
    "    alberta = pd.read_csv('sources/alberta.csv')\n",
    "    alberta = alberta.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    alberta[\"start_date\"] = pd.to_datetime(alberta[\"start_date\"])\n",
    "    \n",
    "    largest_date = alberta[\"start_date\"].max()        \n",
    "    new_additions = load_alberta(since=largest_date)  \n",
    "        \n",
    "    df = alberta.append(new_additions)\n",
    "    df.to_csv('sources/alberta.csv')\n",
    "    return df.drop_duplicates(['source_full_text'], keep=False)\n",
    "\n",
    "def update_manitoba():\n",
    "    manitoba = pd.read_csv('sources/manitoba.csv')\n",
    "    manitoba = manitoba.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    manitoba[\"start_date\"] = pd.to_datetime(manitoba[\"start_date\"])\n",
    "    \n",
    "    largest_date = manitoba[\"start_date\"].max()        \n",
    "    new_additions = load_manitoba(since=largest_date)  \n",
    "        \n",
    "    df = manitoba.append(new_additions)\n",
    "    df.to_csv('sources/manitoba.csv')\n",
    "    return df.drop_duplicates(['source_full_text'], keep=False)\n",
    "\n",
    "def update_pei():\n",
    "    pei = pd.read_csv('sources/pei.csv')\n",
    "    pei = pei.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    pei[\"start_date\"] = pd.to_datetime(pei[\"start_date\"])\n",
    "    \n",
    "    largest_date = pei[\"start_date\"].max()        \n",
    "    new_additions = load_pei(since=largest_date)  \n",
    "        \n",
    "    df = pei.append(new_additions)\n",
    "    df.to_csv('sources/pei.csv')\n",
    "    return df.drop_duplicates(['source_full_text'], keep=False)\n",
    "\n",
    "def update_british_columbia():\n",
    "    british_columbia = pd.read_csv('sources/britishcolumbia.csv')\n",
    "    british_columbia = british_columbia.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    british_columbia[\"start_date\"] = pd.to_datetime(british_columbia[\"start_date\"])\n",
    "    \n",
    "    largest_date = british_columbia[\"start_date\"].max()        \n",
    "    new_additions = load_british_columbia(since=largest_date)  \n",
    "        \n",
    "    df = british_columbia.append(new_additions)\n",
    "    df.to_csv('sources/britishcolumbia.csv')\n",
    "    return df.drop_duplicates(['source_full_text'], keep=False)\n",
    "\n",
    "def update_nunavut():\n",
    "    nunavut = pd.read_csv('sources/nunavut.csv')\n",
    "    nunavut = nunavut.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    nunavut[\"start_date\"] = pd.to_datetime(nunavut[\"start_date\"])\n",
    "    \n",
    "    largest_date = nunavut[\"start_date\"].max()        \n",
    "    new_additions = load_nunavut(since=largest_date)  \n",
    "        \n",
    "    df = nunavut.append(new_additions)\n",
    "    df.to_csv('sources/nunavut.csv')\n",
    "    return df.drop_duplicates(['source_full_text'], keep=False)\n",
    "\n",
    "def update_saskatchewan():\n",
    "    saskatchewan = pd.read_csv('sources/saskatchewan.csv')\n",
    "    saskatchewan = saskatchewan.drop('Unnamed: 0', axis=1) \n",
    "    \n",
    "    saskatchewan[\"start_date\"] = pd.to_datetime(saskatchewan[\"start_date\"])\n",
    "    \n",
    "    largest_date = saskatchewan[\"start_date\"].max()        \n",
    "    new_additions = load_saskatchewan(since=largest_date)  \n",
    "        \n",
    "    df = saskatchewan.append(new_additions)\n",
    "    df.to_csv('sources/saskatchewan.csv')\n",
    "    return df.drop_duplicates(['source_full_text'], keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = update_saskatchewan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
