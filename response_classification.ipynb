{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Classification with LDA and MLPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source_scraping import load_all, load_province\n",
    "from sklearn.utils import shuffle\n",
    "from topic_modelling import *\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.linear_model import SGDClassifier, SGDRegressor, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.svm import LinearSVC, SVC, SVR, LinearSVR\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.optimizers import SGD, Nadam, Adam, Adamax\n",
    "from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy, AUC\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('SCHEMA UPDATE - Collecting NPIs Effects - FULL.csv')\n",
    "pos_df = df.dropna(subset=['oxford_government_response_category', 'source_full_text'])\n",
    "categories = pd.unique([string[0] for string in resp_df['oxford_government_response_category']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Document into Topic Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lda_preprocess(texts, lda_model, lda_dict, stop_words=stopwords.words('english'), allowed_postags=['NOUN', 'ADJ', 'VERB']):\n",
    "    partially_processed = custom_preprocess(texts, stop_words=stop_words, allowed_postags=allowed_postags)\n",
    "    corpus = form_corpus(partially_processed, lda_dict)\n",
    "    texts_by_topic = [lda_model.get_document_topics(doc) for doc in corpus]\n",
    "    processed_texts = []\n",
    "    for topic_list in texts_by_topic:\n",
    "        feature_list = np.zeros(len(lda_model.get_topics()))\n",
    "        for index, value in topic_list:\n",
    "            feature_list[index] = value\n",
    "        processed_texts.append(feature_list)\n",
    "    return np.array(processed_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographical Words to Consider Removing\n",
    "\n",
    "Could be improved: Ontarian might not be removed along with Ontario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_stop_words(df):\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist] # flatten code from stackoverflow...\n",
    "    region_stop_words = set(flatten([reg.lower().split() for reg in df['region'].dropna()]))\n",
    "    sub_region_stop_words = set(flatten([reg.lower().split() for reg in df['subregion'].dropna()]))\n",
    "    geo_stop_words = region_stop_words.union(sub_region_stop_words)\n",
    "    return geo_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_cats(df):\n",
    "    text_to_cats = { }\n",
    "\n",
    "    for index, row in df.dropna(subset=['oxford_government_response_category', 'source_full_text']).iterrows():\n",
    "        ox_cat = row['oxford_government_response_category']\n",
    "        if ox_cat:\n",
    "            text = row['source_full_text']\n",
    "            if text in text_to_cats:\n",
    "                text_to_cats[text].append(ox_cat)\n",
    "            else:\n",
    "                text_to_cats[text] = [ox_cat]\n",
    "                \n",
    "    return text_to_cats\n",
    "\n",
    "def vector_to_cats(v):\n",
    "    return categories[np.nonzero(v)]\n",
    "\n",
    "def process_cats(cats):\n",
    "    one_hot_cat = lambda x : (categories == x[0]).astype(np.float32)\n",
    "    one_hot_arr_cat = lambda a : sum([one_hot_cat(cat) for cat in a])\n",
    "    vectorized = lambda y : [one_hot_arr_cat(arr) for arr in y]\n",
    "    cap = lambda arr : [el if el < 1 else 1 for el in arr]\n",
    "    \n",
    "    return np.array([cap(el) for el in vectorized(cats)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model accuracy on instances that fall into multiple categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_class_accuracy(y_true, y_pred):\n",
    "    num_correct = 0\n",
    "    num_present = 0\n",
    "    for i in range(0, y_true.shape[0]):\n",
    "        v = y_true[i]\n",
    "        if np.count_nonzero(v) > 1:\n",
    "            num_present += 1\n",
    "            if (y_pred[i] == y_true[i]).all():\n",
    "                num_correct += 1\n",
    "                \n",
    "    return num_correct / num_present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split DataFrame into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits data with interventions (i.e. for multilabel classification)\n",
    "\n",
    "def split_npi_data(df, oot=True, multilabel=True):\n",
    "    df = df[df['region'] != 'Quebec']\n",
    "    subsets = ['oxford_government_response_category', 'source_full_text'] if multilabel else ['source_full_text']\n",
    "    df = df.dropna(subset=subsets)\n",
    "    df['start_date'] = pd.to_datetime(df['start_date'])\n",
    "    if oot:\n",
    "        df = df.sort_values(['start_date'])\n",
    "    else:\n",
    "        df = df.sample(frac=1)\n",
    "    \n",
    "    if multilabel:\n",
    "        x_raw = list(text_to_cats(df).keys())\n",
    "        y = list(text_to_cats(df).values())\n",
    "        y = process_cats(y)\n",
    "    else:\n",
    "        x_raw = df['source_full_text']\n",
    "        y = np.array(df['oxford_government_response_category'].notna().astype(np.float))\n",
    "    \n",
    "    return train_test_split(x_raw, y, shuffle=(not oot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_keywords = ['hospital', 'healthcare', 'vaccine', 'trial', 'clinic']\n",
    "e_keywords = ['econom', 'reopen', '$', 'financ', 'financial crisis', 'economic crisis']\n",
    "c_keywords = ['social distanc', 'mask', 'isolation', 'quarantine']\n",
    "\n",
    "def overlap(arr, string):\n",
    "    for word in arr:\n",
    "        if word in string.lower(): return True\n",
    "    return False\n",
    "\n",
    "e = lambda arr : np.array([[overlap(e_keywords, string)] for string in arr])\n",
    "h = lambda arr : np.array([[overlap(h_keywords, string)] for string in arr])\n",
    "c = lambda arr : np.array([[overlap(c_keywords, string)] for string in arr])\n",
    "\n",
    "# Target vectors look like ['H', 'C', 'E']\n",
    "keywords = [h, c, e]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Texts to Input Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_topics(raw_texts, lda_info=None, n_topics=200, stopword_ext=[], keyword_lambdas=[]):\n",
    "    if lda_info is None:\n",
    "        x_train_raw = raw_texts[0]\n",
    "        lda_info = lda_from_list(x_train_raw, n_topic_range=range(n_topics, n_topics + 1), stopword_extensions=stopword_ext, use_coherence=False, plot=False)\n",
    "    \n",
    "    lda_model = lda_info['best_model']\n",
    "    id2word = lda_info['id2word']\n",
    "    \n",
    "    topic_features = []\n",
    "    keyword_features = []\n",
    "    total_features = []\n",
    "        \n",
    "    # raw_texts is a list of lists of documents (list of list of strings)\n",
    "    for text in raw_texts:\n",
    "        topic_vals = lda_preprocess(text, lda_model, id2word)\n",
    "        topic_features.append(topic_vals)\n",
    "        \n",
    "        if keyword_lambdas:\n",
    "            keyword_vals = np.concatenate([kwl(text) for kwl in keyword_lambdas], axis=1).astype(np.float)\n",
    "            keyword_features.append(keyword_vals)\n",
    "\n",
    "            total_vals = np.concatenate((topic_vals, keyword_vals), axis=1)\n",
    "            total_features.append(total_vals)\n",
    "   \n",
    "    if not keyword_lambdas:\n",
    "        total_features = topic_features\n",
    "    total_features.append(lda_info) # For later use\n",
    "    \n",
    "    return total_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Prediction on Unseen Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, lda_info, new_texts, keywords_lambdas=[], stopword_ext=[]):\n",
    "    new_texts_processed, lda_info = text_to_topics([new_texts], lda_info, stopword_ext=stopword_ext, keyword_lambdas=keywords_lambdas)\n",
    "    return model.predict(new_texts_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting\n",
    "\n",
    "Essentially a wrapper for Keras' <code>model.fit()</code>, this function streamlines some of the setup, like TensorBoard setup and an early stopping callback (piece of code called at the end of each epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, x_train, y_train, epochs=200, validation_split=0.2, validation_data=None, log=True, name=''):\n",
    "    import time\n",
    "\n",
    "    root_logdir = os.path.join(os.curdir, \"tb_logs\")\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    run_logdir = os.path.join(root_logdir, run_id + name)\n",
    "    \n",
    "    tensobroad_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "    early_stop = keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True)\n",
    "    callbacks = [early_stop, tensobroad_cb] if log else [early_stop]\n",
    "    \n",
    "    return model.fit(x_train, y_train, epochs=epochs, validation_split=validation_split, callbacks=callbacks, validation_data=validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = Sequential([\n",
    "        Dense(200, name='dense_200'),\n",
    "        Dropout(0.5, name='first_dropout_0.5'),\n",
    "        Dense(80, name='dense_80'),\n",
    "        Dropout(0.5, name='second_dropout_0.5'),\n",
    "#         Dense(80),\n",
    "#         Dropout(0.5), # Seems unnecessary\n",
    "        Dense(10, name='dense_10'),\n",
    "        Dense(3, name='output_3', activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "mlp_model.compile(optimizer=Nadam(), loss='binary_crossentropy', metrics=[AUC(curve='roc'), BinaryAccuracy(), Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_raw, x_test_raw, y_train, y_test = split_npi_data(df, oot=True)\n",
    "x_train, x_test, lda_info = text_to_topics((x_train_raw, x_test_raw), n_topics=200, keyword_lambdas=keywords)\n",
    "x_train, y_train = shuffle(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "WARNING:tensorflow:Layer dense_200 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "19/19 [==============================] - 1s 29ms/step - loss: 0.6628 - auc_57: 0.5986 - binary_accuracy: 0.6332 - precision_57: 0.4892 - recall_57: 0.3839 - val_loss: 0.6154 - val_auc_57: 0.7210 - val_binary_accuracy: 0.7089 - val_precision_57: 0.6216 - val_recall_57: 0.4367\n",
      "Epoch 2/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.6052 - auc_57: 0.7045 - binary_accuracy: 0.7097 - precision_57: 0.6529 - recall_57: 0.4164 - val_loss: 0.5697 - val_auc_57: 0.7698 - val_binary_accuracy: 0.7089 - val_precision_57: 0.6080 - val_recall_57: 0.4810\n",
      "Epoch 3/200\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.5577 - auc_57: 0.7729 - binary_accuracy: 0.7393 - precision_57: 0.6692 - recall_57: 0.5480 - val_loss: 0.5314 - val_auc_57: 0.8096 - val_binary_accuracy: 0.7444 - val_precision_57: 0.6617 - val_recall_57: 0.5570\n",
      "Epoch 4/200\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.5212 - auc_57: 0.8094 - binary_accuracy: 0.7571 - precision_57: 0.7025 - recall_57: 0.5666 - val_loss: 0.4938 - val_auc_57: 0.8398 - val_binary_accuracy: 0.7689 - val_precision_57: 0.6929 - val_recall_57: 0.6139\n",
      "Epoch 5/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.4978 - auc_57: 0.8274 - binary_accuracy: 0.7672 - precision_57: 0.7041 - recall_57: 0.6115 - val_loss: 0.4609 - val_auc_57: 0.8613 - val_binary_accuracy: 0.7844 - val_precision_57: 0.7075 - val_recall_57: 0.6582\n",
      "Epoch 6/200\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.4745 - auc_57: 0.8417 - binary_accuracy: 0.7867 - precision_57: 0.7324 - recall_57: 0.6440 - val_loss: 0.4310 - val_auc_57: 0.8807 - val_binary_accuracy: 0.7956 - val_precision_57: 0.7324 - val_recall_57: 0.6582\n",
      "Epoch 7/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.4526 - auc_57: 0.8594 - binary_accuracy: 0.7873 - precision_57: 0.7304 - recall_57: 0.6502 - val_loss: 0.4079 - val_auc_57: 0.8954 - val_binary_accuracy: 0.8156 - val_precision_57: 0.7660 - val_recall_57: 0.6835\n",
      "Epoch 8/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.4246 - auc_57: 0.8772 - binary_accuracy: 0.8023 - precision_57: 0.7475 - recall_57: 0.6827 - val_loss: 0.3969 - val_auc_57: 0.8951 - val_binary_accuracy: 0.8222 - val_precision_57: 0.7635 - val_recall_57: 0.7152\n",
      "Epoch 9/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.4192 - auc_57: 0.8808 - binary_accuracy: 0.8096 - precision_57: 0.7512 - recall_57: 0.7059 - val_loss: 0.3764 - val_auc_57: 0.9079 - val_binary_accuracy: 0.8333 - val_precision_57: 0.7902 - val_recall_57: 0.7152\n",
      "Epoch 10/200\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.3926 - auc_57: 0.8962 - binary_accuracy: 0.8303 - precision_57: 0.7908 - recall_57: 0.7198 - val_loss: 0.3688 - val_auc_57: 0.9102 - val_binary_accuracy: 0.8333 - val_precision_57: 0.7785 - val_recall_57: 0.7342\n",
      "Epoch 11/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.3956 - auc_57: 0.8947 - binary_accuracy: 0.8252 - precision_57: 0.7707 - recall_57: 0.7337 - val_loss: 0.3650 - val_auc_57: 0.9105 - val_binary_accuracy: 0.8356 - val_precision_57: 0.7800 - val_recall_57: 0.7405\n",
      "Epoch 12/200\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.3852 - auc_57: 0.9009 - binary_accuracy: 0.8202 - precision_57: 0.7647 - recall_57: 0.7245 - val_loss: 0.3629 - val_auc_57: 0.9114 - val_binary_accuracy: 0.8422 - val_precision_57: 0.7959 - val_recall_57: 0.7405\n",
      "Epoch 13/200\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.3742 - auc_57: 0.9042 - binary_accuracy: 0.8319 - precision_57: 0.7814 - recall_57: 0.7415 - val_loss: 0.3606 - val_auc_57: 0.9116 - val_binary_accuracy: 0.8489 - val_precision_57: 0.8000 - val_recall_57: 0.7595\n",
      "Epoch 14/200\n",
      "19/19 [==============================] - 0s 9ms/step - loss: 0.3674 - auc_57: 0.9092 - binary_accuracy: 0.8375 - precision_57: 0.7831 - recall_57: 0.7601 - val_loss: 0.3632 - val_auc_57: 0.9089 - val_binary_accuracy: 0.8356 - val_precision_57: 0.7877 - val_recall_57: 0.7278\n",
      "Epoch 15/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.3786 - auc_57: 0.9013 - binary_accuracy: 0.8247 - precision_57: 0.7776 - recall_57: 0.7198 - val_loss: 0.3664 - val_auc_57: 0.9064 - val_binary_accuracy: 0.8356 - val_precision_57: 0.7877 - val_recall_57: 0.7278\n",
      "Epoch 16/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.3606 - auc_57: 0.9115 - binary_accuracy: 0.8331 - precision_57: 0.7868 - recall_57: 0.7368 - val_loss: 0.3713 - val_auc_57: 0.9076 - val_binary_accuracy: 0.8333 - val_precision_57: 0.7712 - val_recall_57: 0.7468\n",
      "Epoch 17/200\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.3548 - auc_57: 0.9136 - binary_accuracy: 0.8414 - precision_57: 0.7948 - recall_57: 0.7554 - val_loss: 0.3619 - val_auc_57: 0.9121 - val_binary_accuracy: 0.8333 - val_precision_57: 0.7748 - val_recall_57: 0.7405\n",
      "Epoch 18/200\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.3521 - auc_57: 0.9163 - binary_accuracy: 0.8448 - precision_57: 0.7977 - recall_57: 0.7632 - val_loss: 0.3611 - val_auc_57: 0.9118 - val_binary_accuracy: 0.8378 - val_precision_57: 0.7891 - val_recall_57: 0.7342\n",
      "Epoch 19/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.3432 - auc_57: 0.9197 - binary_accuracy: 0.8409 - precision_57: 0.7888 - recall_57: 0.7632 - val_loss: 0.3704 - val_auc_57: 0.9077 - val_binary_accuracy: 0.8378 - val_precision_57: 0.7891 - val_recall_57: 0.7342\n",
      "Epoch 20/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.3483 - auc_57: 0.9171 - binary_accuracy: 0.8375 - precision_57: 0.7795 - recall_57: 0.7663 - val_loss: 0.3763 - val_auc_57: 0.9051 - val_binary_accuracy: 0.8356 - val_precision_57: 0.7800 - val_recall_57: 0.7405\n",
      "Epoch 21/200\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.3439 - auc_57: 0.9193 - binary_accuracy: 0.8431 - precision_57: 0.7958 - recall_57: 0.7601 - val_loss: 0.3722 - val_auc_57: 0.9086 - val_binary_accuracy: 0.8333 - val_precision_57: 0.7785 - val_recall_57: 0.7342\n",
      "Epoch 22/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.3370 - auc_57: 0.9237 - binary_accuracy: 0.8548 - precision_57: 0.8103 - recall_57: 0.7802 - val_loss: 0.3737 - val_auc_57: 0.9074 - val_binary_accuracy: 0.8289 - val_precision_57: 0.7755 - val_recall_57: 0.7215\n",
      "Epoch 23/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.3450 - auc_57: 0.9194 - binary_accuracy: 0.8520 - precision_57: 0.8019 - recall_57: 0.7833 - val_loss: 0.3742 - val_auc_57: 0.9073 - val_binary_accuracy: 0.8311 - val_precision_57: 0.7733 - val_recall_57: 0.7342\n",
      "Epoch 24/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.3294 - auc_57: 0.9277 - binary_accuracy: 0.8582 - precision_57: 0.8267 - recall_57: 0.7678 - val_loss: 0.3798 - val_auc_57: 0.9058 - val_binary_accuracy: 0.8311 - val_precision_57: 0.7733 - val_recall_57: 0.7342\n",
      "Epoch 25/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.3324 - auc_57: 0.9259 - binary_accuracy: 0.8543 - precision_57: 0.8090 - recall_57: 0.7802 - val_loss: 0.3788 - val_auc_57: 0.9058 - val_binary_accuracy: 0.8311 - val_precision_57: 0.7808 - val_recall_57: 0.7215\n",
      "Epoch 26/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.3253 - auc_57: 0.9286 - binary_accuracy: 0.8587 - precision_57: 0.8248 - recall_57: 0.7724 - val_loss: 0.3802 - val_auc_57: 0.9064 - val_binary_accuracy: 0.8311 - val_precision_57: 0.7733 - val_recall_57: 0.7342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.3361 - auc_57: 0.9234 - binary_accuracy: 0.8504 - precision_57: 0.8048 - recall_57: 0.7724 - val_loss: 0.3825 - val_auc_57: 0.9060 - val_binary_accuracy: 0.8289 - val_precision_57: 0.7718 - val_recall_57: 0.7278\n",
      "Epoch 28/200\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.3273 - auc_57: 0.9279 - binary_accuracy: 0.8504 - precision_57: 0.8078 - recall_57: 0.7678 - val_loss: 0.3940 - val_auc_57: 0.8983 - val_binary_accuracy: 0.8200 - val_precision_57: 0.7584 - val_recall_57: 0.7152\n",
      "Epoch 29/200\n",
      "19/19 [==============================] - 0s 6ms/step - loss: 0.3284 - auc_57: 0.9274 - binary_accuracy: 0.8498 - precision_57: 0.8035 - recall_57: 0.7724 - val_loss: 0.3797 - val_auc_57: 0.9068 - val_binary_accuracy: 0.8333 - val_precision_57: 0.7823 - val_recall_57: 0.7278\n",
      "Epoch 30/200\n",
      "19/19 [==============================] - 0s 8ms/step - loss: 0.3281 - auc_57: 0.9272 - binary_accuracy: 0.8654 - precision_57: 0.8230 - recall_57: 0.7988 - val_loss: 0.3835 - val_auc_57: 0.9054 - val_binary_accuracy: 0.8333 - val_precision_57: 0.7785 - val_recall_57: 0.7342\n",
      "Epoch 31/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.3191 - auc_57: 0.9311 - binary_accuracy: 0.8565 - precision_57: 0.8152 - recall_57: 0.7786 - val_loss: 0.3910 - val_auc_57: 0.9029 - val_binary_accuracy: 0.8311 - val_precision_57: 0.7770 - val_recall_57: 0.7278\n",
      "Epoch 32/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.3208 - auc_57: 0.9310 - binary_accuracy: 0.8571 - precision_57: 0.8105 - recall_57: 0.7879 - val_loss: 0.3892 - val_auc_57: 0.9036 - val_binary_accuracy: 0.8333 - val_precision_57: 0.7823 - val_recall_57: 0.7278\n",
      "Epoch 33/200\n",
      "19/19 [==============================] - 0s 7ms/step - loss: 0.3160 - auc_57: 0.9330 - binary_accuracy: 0.8565 - precision_57: 0.8082 - recall_57: 0.7895 - val_loss: 0.3948 - val_auc_57: 0.9032 - val_binary_accuracy: 0.8267 - val_precision_57: 0.7667 - val_recall_57: 0.7278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14dd5adc0>"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_model(mlp_model, x_train, y_train, epochs=200, validation_split=0.2, log=False, name='...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 5ms/step - loss: 0.3309 - auc_57: 0.9324 - binary_accuracy: 0.8755 - precision_57: 0.8238 - recall_57: 0.8008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.33094245195388794,\n",
       " 0.9323994517326355,\n",
       " 0.8755020499229431,\n",
       " 0.8237704634666443,\n",
       " 0.8007968068122864]"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7349397590361446"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "y_pred = (mlp_model.predict(x_test) > threshold).astype(np.float32)\n",
    "x, y = np.concatenate((x_train, x_test), axis=0), np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "accuracy_score(y_test, y_pred), multi_class_accuracy(y, (mlp_model.predict(x) > 0.5).astype(np.float32)) # :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 = load_all(verbose=True)\n",
    "# df2_en = df2[df2['region'] != 'Quebec']\n",
    "# df2_en = df2_en.sample(frac=1)\n",
    "# df2_en.to_csv('full_release_set_en.csv')\n",
    "new = df2_en['source_full_text'][:31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df_2019 = load_all(start_date=datetime(2019, 10, 1), end_date=datetime(2019, 11, 1), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019 = df_2019[df_2019['start_date'] <= datetime(2019, 11, 1)]\n",
    "full_df = df_2019.append(pos_df).sample(frac=1)\n",
    "x_train_raw, x_test_raw, y_train, y_test = split_npi_data(full_df, multilabel=False, oot=False)\n",
    "x_train, x_test, lda_info = text_to_topics((x_train_raw, x_test_raw), lda_info=lda_info, n_topics=200, keyword_lambdas=keywords)\n",
    "x_train, y_train = shuffle(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
       "       1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
       "       1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
       "       1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
       "       1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
       "       1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
       "       0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
       "       0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
       "       1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
       "       1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
       "       0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
       "       0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "       0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
       "       1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
       "       1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
       "       0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
       "       1., 1.])"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RND_CLF\n",
      "Accuracy 0.8896551724137931\n",
      "Precision 0.9237804878048781\n",
      "Recall 0.8859649122807017\n",
      "\n",
      "\n",
      "LIN_SVC_CLF\n",
      "Accuracy 0.8362068965517241\n",
      "Precision 0.8823529411764706\n",
      "Recall 0.8333333333333334\n",
      "\n",
      "\n",
      "RBF_SVC_CLF\n",
      "Accuracy 0.8275862068965517\n",
      "Precision 0.8829113924050633\n",
      "Recall 0.8157894736842105\n",
      "\n",
      "\n",
      "KNN_CLF\n",
      "Accuracy 0.8362068965517241\n",
      "Precision 0.8776758409785933\n",
      "Recall 0.8391812865497076\n",
      "\n",
      "\n",
      "LOG_CLF\n",
      "Accuracy 0.7982758620689655\n",
      "Precision 0.8440366972477065\n",
      "Recall 0.8070175438596491\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    'rnd_clf' : RandomForestClassifier(n_estimators=400, max_depth=80),\n",
    "    'lin_svc_clf' : LinearSVC(),\n",
    "    'rbf_svc_clf' : SVC(kernel='rbf'),\n",
    "    'knn_clf' : KNeighborsClassifier(),\n",
    "    'log_clf' : LogisticRegression()\n",
    "}\n",
    "\n",
    "metrics = {\n",
    "    'Accuracy' : accuracy_score,\n",
    "    'Precision' : precision_score,\n",
    "    'Recall' : recall_score\n",
    "#     'ROC AUC' : roc_auc_score\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(x_train, y_train)\n",
    "    print(name.upper())\n",
    "    for metric, metric_func in metrics.items():\n",
    "        print(metric, metric_func(y_test, model.predict(x_test)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf = Sequential([\n",
    "        Dense(200, name='dense_200', activation='elu'),\n",
    "        Dropout(0.5, name='first_dropout_0.5'),\n",
    "        Dense(80, name='dense_80', activation='elu'),\n",
    "        Dropout(0.5, name='second_dropout_0.5'),\n",
    "        Dense(10, name='dense_10', activation='elu'),\n",
    "        Dense(1, name='output_3', activation='sigmoid')\n",
    "])\n",
    "\n",
    "mlp_clf.compile(optimizer=Nadam(), loss='binary_crossentropy', metrics=[AUC(curve='roc'), BinaryAccuracy(), Precision(), Recall()])\n",
    "fit_model(mlp_clf, x_train, y_train, epochs=200, validation_split=0.2, log=False, name='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 3ms/step - loss: 0.3489 - auc_59: 0.9198 - binary_accuracy: 0.8379 - precision_59: 0.8584 - recall_59: 0.8684\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3489212095737457,\n",
       " 0.9198486804962158,\n",
       " 0.8379310369491577,\n",
       " 0.8583815097808838,\n",
       " 0.8684210777282715]"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_clf.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models['rnd_clf']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python38264bitb7deb3926a5743b595e3b44ad861ac00"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
